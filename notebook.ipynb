{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TODO\n",
    "seeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup\n",
    "## Variables for configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoded_features = ['cp','restecg', 'slope','ca', 'restwm']\n",
    "# list the datasets that should be used in the current run\n",
    "datasets = [\"hungarian\", \"cleveland\", \"switzerland\", \"long-beach-va\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "# set pandas to show all columns of the df when using the display function\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function to read the dataset into a csv formatted string\n",
    "# the name is used as a delimiter here because it is the last feature and common among all entrys \n",
    "def read_raw_data(file_path:str):\n",
    "    with open(file_path) as file:\n",
    "        file_string = file.read()\n",
    "        # remove unnecessary linebreaks\n",
    "        file_string = file_string.replace(\"\\n\",\" \")\n",
    "        # break lines after name to separate measurements by line (name is a constant and the last attribute)\n",
    "        file_string = file_string.replace(\"name \",\"name\\n\")\n",
    "        # separate columns by \",\" instead of \" \".\n",
    "        file_string = file_string.replace(\" \",\",\")\n",
    "        return file_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the specified datasets into the df \n",
    "from io import StringIO\n",
    "df = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    dataset_df = pd.read_csv(StringIO(read_raw_data(\"./Data/\"+ dataset +\".data\")), header=None, sep=\",\")\n",
    "    dataset_df['dataset'] = dataset\n",
    "    df = pd.concat([df,dataset_df ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[\"id\", \"ccf\", \"age\", \"sex\", \"painloc\", \"painexer\" , \"relrest\" , \"pncaden\" , \"cp\", \"trestbps\", \"htn\", \"chol\", \"smoke\", \"cigs\", \"years\", \"fbs\", \"dm\", \"famhist\", \"restecg\", \"ekgmo\", \"ekgday\", \"ekgyr\", \"dig\", \"prop\", \"nitr\", \"pro\", \"diuretic\", \"proto\", \"thaldur\", \"thaltime\", \"met\", \"thalach\", \"thalrest\", \"tpeakbps\", \"tpeakbpd\", \"dummy\", \"trestbpd\", \"exang\", \"xhypo\", \"oldpeak\", \"slope\", \"rldv5\", \"rldv5e\", \"ca\", \"restckm\", \"exerckm\", \"restef\", \"restwm\", \"exeref\", \"exerwm\", \"thal\", \"thalsev\", \"thalpul\", \"earlobe\", \"cmo\", \"cday\", \"cyr\", \"num\", \"lmt\", \"ladprox\", \"laddist\", \"diag\", \"cxmain\", \"ramus\", \"om1\", \"om2\", \"rcaprox\", \"rcadist\", \"lvx1\", \"lvx2\", \"lvx3\", \"lvx4\", \"lvf\", \"cathef\", \"junk\", \"name\", \"dataset\"]\n",
    "# Trestbps and trestbpd describe resting blood pressure but are not highly correlated. Therefore, we keep them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replace -9 by NaN (according to Data/heart-disease.names)\n",
    "df.replace(-9,np.float64(\"NaN\"), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we want to predict whether a patient has any heart disease, not the type/degree of heart disease as recommended by the UCI https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "df[df[\"num\"]>1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treatment of different scales in the datasets\n",
    "the reasons for this processing are laid out further in the analysis notebook\n",
    "## met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plot to show the reason why we need to process this data:\n",
    "sns.boxplot(x=\"met\",y=\"dataset\",data= df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# the measurements of switzerland do not seem valid -> replace with NaN\n",
    "df.loc[df[\"dataset\"] == \"switzerland\", \"met\"] = np.float64(\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rldv5e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to show the reason why we need to process this data: \n",
    "sns.boxplot(x=\"rldv5e\",y=\"dataset\",data= df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the measurements of cleveland do not seem valid -> replace with NaN\n",
    "df.loc[df[\"dataset\"] == \"cleveland\", \"rldv5e\"] = np.float64(\"NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "irrelevant_columns = [\n",
    "    \"id\", # A id is not relevant for a model\n",
    "    \"ccf\", # the social security number does not influence if you have a heart disease or not\n",
    "    \"pncaden\", # sum of painlox painexer relrest -> the features are already in the dataset -> drop because it is a duplicate\n",
    "    \"ekgmo\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"ekgday\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"ekgyr\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"cmo\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"cday\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"cyr\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"name\" # Constant\n",
    "]\n",
    "df.drop(irrelevant_columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "unexplained_columns = [\n",
    "    \"restckm\", # irrelevant according to the uci\n",
    "    \"exerckm\", # irrelevant according to the uci\n",
    "    \"thalsev\", # irrelevant according to the uci\n",
    "    \"thalpul\", # irrelevant according to the uci\n",
    "    \"earlobe\", # Constant\n",
    "    \"lvx1\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"lvx2\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"lvx3\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"lvx4\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"lvf\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    'junk', # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"dummy\" # same as trestbps\n",
    "]\n",
    "df.drop(unexplained_columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "hidden_identifier = [\n",
    "    'lmt',      # Left main truck\n",
    "    'ladprox',  # Proximal left anterior descending artery\n",
    "    'laddist',  # Distal left anterior descending artery\n",
    "    'diag',     # Diagonal branches\n",
    "    'cxmain',   # Circumflex\n",
    "    'ramus',    # Ramus intermedius\n",
    "    'om1',      # First obtuse marginal branch\n",
    "    'om2',      # Second obtuse marginal branch\n",
    "    'rcaprox',  # Proximal right coronary artery\n",
    "    'rcadist',  # Distal right coronary artery\n",
    "]\n",
    "df.drop(hidden_identifier, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from  sklearn.naive_bayes import *\n",
    "\n",
    "estimators=[\n",
    "    {\"estimator\": XGBClassifier(random_state=42, n_jobs=1), \"parameters\": {}},\n",
    "                                    # 'estimator__max_depth': [None] + [2,6,10,20,50,100],\n",
    "                                    # 'estimator__n_estimators': range(10,1000, 100),           Not enough time and heuristics of XGBoost are very good\n",
    "                                    # 'estimator__learning_rate':[0.001,0.01,0.1,0.2,0.3]}\n",
    "    {\"estimator\": SVC(random_state=42, tol=0.01), \"parameters\": {'estimator__C': [120,130,140,160],\n",
    "                                                                 'estimator__gamma': [0.0001, 0.001, 0.01],\n",
    "                                                                 'estimator__kernel':['linear', 'rbf', 'poly', 'sigmoid'] }},\n",
    "    {\"estimator\": BernoulliNB(), \"parameters\": {'estimator__alpha' : np.arange(0,20,0.1)}},\n",
    "    {\"estimator\": CategoricalNB(), \"parameters\": {'estimator__alpha' : np.arange(0,20,0.1)}},\n",
    "    {\"estimator\": ComplementNB(), \"parameters\": {'estimator__alpha' : np.arange(0,20,0.1),\n",
    "                                                 'estimator__norm':[True,False]}},\n",
    "    {\"estimator\": GaussianNB(), \"parameters\": {}},\n",
    "    {\"estimator\": MultinomialNB(), \"parameters\": {'estimator__alpha' : np.arange(0,20,0.1)}},\n",
    "    {\"estimator\": DecisionTreeClassifier(random_state=42), \"parameters\": {'estimator__criterion':['gini','entropy'],\n",
    "                                                                          'estimator__max_depth':[None,2,6,10],\n",
    "                                                                          'estimator__min_samples_split': range(2,11,4)}},\n",
    "    {\"estimator\": KNeighborsClassifier(), \"parameters\": {'estimator__n_neighbors': range(2, 100,5),\n",
    "                                                         'estimator__weights': ['uniform','distance'],\n",
    "                                                         'estimator__p': [1,2]}},\n",
    "    {\"estimator\": RandomForestClassifier(random_state=42, n_jobs=1), \"parameters\": {'estimator__n_estimators':range(10,100, 10),\n",
    "                                                                                    'estimator__max_depth':[None,2,6,10],\n",
    "                                                                                    'estimator__min_samples_split': range(2,11,4)\n",
    "                                                                                   }},\n",
    "    {\"estimator\": LogisticRegression(solver='liblinear', max_iter=10000000), \"parameters\": {'estimator__penalty':['l1','l2']}}\n",
    "    # {\"estimator\": SGDClassifier(max_iter=100000), \"parameters\": {'estimator__loss':['log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    #                                               'estimator__penalty':['l1','l2','elasticnet'],\n",
    "    #                                               'estimator__alpha' : np.arange(1,40,5)}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *\n",
    "scalers = [\n",
    "    {\"scaler\": MaxAbsScaler(), \"parameters\": {}},\n",
    "    {\"scaler\": MinMaxScaler(), \"parameters\": {}},\n",
    "    {\"scaler\": Normalizer(), \"parameters\": {'scaler__norm': ['l1', 'l2', 'max']}},\n",
    "    {\"scaler\": PowerTransformer(), \"parameters\": {}},\n",
    "    {\"scaler\": RobustScaler(), \"parameters\": {}},\n",
    "    {\"scaler\": StandardScaler(), \"parameters\": {}},\n",
    "    {\"scaler\": 'passthrough', \"parameters\": {}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "imputers = [\n",
    "    {\"imputer\": SimpleImputer(), \"parameters\": {'impute__strategy' : ['mean', 'median', 'most_frequent']}},\n",
    "    # KNN imputer is not used after inspection of the runtime with the KNN classifier (see KNN_classifier_with_KNN_and_simple_imputer.json)\n",
    "    # {\"imputer\": KNNImputer(), \"parameters\": {'impute__n_neighbors': range(2, 10,1)}},\n",
    "    # iterative imputer is not used because bugs were observed during the usage of this experimental feature\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "samplers = ['passthrough', RandomOverSampler(random_state=42),RandomUnderSampler(random_state=42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "general_parameters = {\n",
    "    #values are selected based on analysis in Analyse.ipynb\n",
    "    'drop_columns__minimum_percentage_to_be_dropped': [0,4,8,20,35,60,75,100],\n",
    "    'oneHotEncoder__discretize':[\n",
    "        'passthrough',\n",
    "        KBinsDiscretizer(2,encode='ordinal', strategy='uniform'),\n",
    "        KBinsDiscretizer(5,encode='ordinal', strategy='uniform')]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns smoke, cigs and years describe whether a respondent smokes or not. Smoke does this by being binary coded, while years describes the number of years a person has smoked. Cigs describes how many cigarettes the person smokes a day. Due to the high number of missing values in smoke, it is enriched with the years and cigs columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DataframeSmokeTransformer:\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        # we do not enrich smoke if cigs and years are conflicting\n",
    "\n",
    "        # set all values of smoke that are NaN to 0 if the value of years is 0 and smoke does not indicate that the person smokes\n",
    "        input_df.loc[(input_df['smoke'].isna()) & (input_df['years'] == 0) & ~(input_df['cigs'] > 0),'smoke'] = 0\n",
    "        # set all values of smoke that are NaN to 1 if the value of years is larger than 0 and smoke does not indicate that the person does not smoke\n",
    "        input_df.loc[(input_df['smoke'].isna()) & (input_df['years'] > 0) & (input_df['cigs'] != 0),'smoke'] = 1\n",
    "\n",
    "        # set all values of smoke that are NaN to 0 if the value of smoke is 0 and years does not indicate that the person smokes\n",
    "        input_df.loc[(input_df['smoke'].isna()) & (input_df['cigs'] == 0) & ~(input_df['years'] > 0),'smoke'] = 0\n",
    "        # set all values of smoke that are NaN to 1 if the value of cigs is larger than 0 and years does not indicate that the person does not smoke\n",
    "        input_df.loc[(input_df['smoke'].isna()) & (input_df['cigs'] > 0) & (input_df['years'] != 0),'smoke'] = 1\n",
    "        return input_df\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # there is nothing to be fitted here because this handling is not split specific\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EnrichHeartData:\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        input_df[\"heart_rate_diff\"] = input_df['thalach'] - input_df['thalrest']\n",
    "        input_df[\"rldv5_diff\"] = input_df['rldv5'] - input_df['rldv5e']\n",
    "        return input_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.exceptions import NotFittedError\n",
    "class DropColumnsBasedOnMinimumPercentageToBeDropped:\n",
    "    def __init__(self):\n",
    "        self.minimum_percentage_to_be_dropped = 100\n",
    "        self.fitted = False\n",
    "        self.valuesToKeep = []\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.minimum_percentage_to_be_dropped = params.get('minimum_percentage_to_be_dropped')\n",
    "\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        if self.fitted:\n",
    "            return input_df[input_df.columns.intersection(self.valuesToKeep)]\n",
    "        else:\n",
    "            raise NotFittedError()\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # calculate percentage of missing values for each column and store in a dictionary\n",
    "        percentage_missing = (X.isna().sum()/len(df)*100).to_dict()\n",
    "        # generate list of columns to keep\n",
    "        self.valuesToKeep = [key for key, val in percentage_missing.items() if val <= self.minimum_percentage_to_be_dropped]\n",
    "        self.fitted = True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FixCommonEncodingErrors:\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        input_df = input_df.copy(deep=True)\n",
    "        # if cholesterin is 0 it was not measured\n",
    "        input_df.loc[input_df['chol'] == 0,'chol'] =  np.float64(\"NaN\")\n",
    "        # leave the dead ones behind\n",
    "        # drop entries with a blood pressure of 0\n",
    "        input_df.loc[input_df['trestbps'] == 0,'trestbps'] =  np.float64(\"NaN\")\n",
    "        # is a binary variable (wrong measurement was detected in Pandas Profiling Report)\n",
    "        input_df.loc[df['prop'].isin([0,1]) == False,'prop' ] = np.float64(\"NaN\")\n",
    "        # is a variable that has the values 0-3 by definition  (wrong measurement was detected in Pandas Profiling Report)\n",
    "        input_df.loc[input_df['ca'] >3 ,'ca'] =  np.float64(\"NaN\")\n",
    "        # transform proto according to possible values from data/ask-detrano\n",
    "        input_df.loc[input_df['proto'] == 200,'proto'] =  9\n",
    "        input_df.loc[input_df['proto'] == 175,'proto'] =  8\n",
    "        input_df.loc[input_df['proto'] == 150,'proto'] =  7\n",
    "        input_df.loc[input_df['proto'] == 130,'proto'] =  6\n",
    "        input_df.loc[input_df['proto'] == 125,'proto'] =  5\n",
    "        input_df.loc[input_df['proto'] == 100,'proto'] = 4\n",
    "        input_df.loc[input_df['proto'] == 75,'proto'] = 3\n",
    "        input_df.loc[input_df['proto'] == 50,'proto'] = 2\n",
    "        input_df.loc[input_df['proto'] == 25,'proto'] = 1\n",
    "        #set all other values to NaN\n",
    "        input_df.loc[input_df['proto'].isin([*range(1,13)]) == False, 'proto'] = np.float64(\"NaN\")\n",
    "        # the timepoint when the measurement was taken can not be larger than the time that the exercise took.\n",
    "        input_df.loc[df['thaltime'] > df['thaldur'], 'thaltime'] = np.float64('NaN')\n",
    "        # maximum archived heart rate can not  be lower than the heart rate at rest\n",
    "        input_df.loc[input_df['thalach'] < input_df['thalrest'],'thalach'] = np.float64('NaN')\n",
    "\n",
    "        return input_df\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# assumption the dictionaries are of equal structure\n",
    "def merge_dict(dict1, dict2):\n",
    "    for key, val in dict1.items():\n",
    "        # merge nested dictionaries\n",
    "        if type(val) == dict:\n",
    "            dict1[key] = merge_dict(dict1[key], dict2[key])\n",
    "        # if value of dict1 is a list -> append values of dict2[key] to that list\n",
    "        elif type(val) == list:\n",
    "            if type(dict2[key]) == list:\n",
    "                dict1[key] = [ *dict1[key], *dict2[key]]\n",
    "            else:\n",
    "                dict1[key] = [*dict1[key], dict2[key]]\n",
    "        else:\n",
    "            # merge values into a new list\n",
    "            dict1[key] = [val, dict2[key]]\n",
    "\n",
    "    return dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# It is necessary to write a separate score function to obtain and save results from the respective cross validations later on (outer loop of nested cv).\n",
    "# This procedure requires working with pickle, as it is not possible to interact with data structures from the notebook (even if they are marked as global), as make_scorer later creates a deepCopy of this function and thus the data structures are also copied. The same applies to the parameters that could be passed to this function in the makeScorer function.\n",
    "def classification_report_with_auc_score(y_true, y_pred):\n",
    "    # calculate the score that will be returned to the cross validation to obtain the optimal hyperparameters\n",
    "    current_roc_auc_score = roc_auc_score(y_true, y_pred)\n",
    "    #transform confusion matrix to dictionary\n",
    "    confusion_matrix_dict = {}\n",
    "    for idxRow, row in np.ndenumerate(confusion_matrix(y_true, y_pred)):\n",
    "        confusion_matrix_dict[str(idxRow)] = row\n",
    "    # check if pickle was created by another cross validation loop\n",
    "    if os.path.exists('temp.pickle'):\n",
    "        with open(\"temp.pickle\", \"rb\") as temp_file:\n",
    "            # read pickled dictionary of previous cross validation loop\n",
    "            report = pickle.load(temp_file)\n",
    "            # append current score\n",
    "            report[\"auc\"].append(current_roc_auc_score)\n",
    "            # merge classification reports\n",
    "            report['classification_report'] = merge_dict(report['classification_report'], classification_report(y_true, y_pred, output_dict=True))\n",
    "            # merge confusion matrix dictionaries\n",
    "            report['confusion_matrix'] = merge_dict(report['confusion_matrix'], confusion_matrix_dict)\n",
    "    else:\n",
    "        #create dictionary for first pickle\n",
    "        report = {'classification_report': classification_report(y_true, y_pred, output_dict=True),\n",
    "                  \"auc\": [current_roc_auc_score],\n",
    "                  'confusion_matrix': confusion_matrix_dict\n",
    "                  }\n",
    "    # write report dictionary to pickle-file\n",
    "    with open('temp.pickle', 'wb') as temp_file:\n",
    "        pickle.dump(report, temp_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # return calculated roc_auc_score for evaluation in cross validation\n",
    "    return current_roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from imblearn.base import BaseSampler\n",
    "import json\n",
    "\n",
    "#custom Encoder for serialization of output\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if type(obj) == range:\n",
    "            return [*obj]\n",
    "        if isinstance(obj, BaseSampler):\n",
    "            return obj.__class__.__name__\n",
    "        if isinstance(obj, KBinsDiscretizer):\n",
    "            return obj.get_params()\n",
    "        if isinstance(obj, ColumnTransformer):\n",
    "            return obj.get_params()\n",
    "        return super(CustomEncoder, self).default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer\n",
    "\n",
    "# separate features from target variable\n",
    "X = df.loc[:,(df.columns!= 'num') & (df.columns != 'dataset') ].copy(deep=True)\n",
    "y = df.loc[: , (df.columns== 'num')].values.ravel().copy()\n",
    "\n",
    "# We would like to be able to analyse the influence of scalers, estimators, imputers and samplers on the score. Therefore, we refrain from using them as hyperparameters. Instead, we loop over all possible configurations and create separate pipelines that are logged separately in the output. In the later analysis, the parameters can still be considered as hyperparamters, but they do not have to be, thus allowing a more detailed analysis. In order to make the results comparable the cross validations and estimators etc. are seeded if possible.\n",
    "for scaler in scalers:\n",
    "    for estimator in estimators:\n",
    "        for imputer in imputers:\n",
    "            for sampler in samplers:\n",
    "\n",
    "                # combine the parameter dictionaries (| is a valid operator because the keys do not overlap)\n",
    "                parameters =  scaler.get(\"parameters\") | estimator.get(\"parameters\") | imputer.get('parameters') | general_parameters\n",
    "                # use column transformer to oneHotEncode features. Because some categorical values have very few occurences, it could happen, that features are not present in the train set but only in test. This would lead to an error if these categories would not be ignored. The columns that are oneHotEncoded are defined at runtime by the given lambda because it could happen that a column that should be oneHotEncoded was dropped in an earlier pipeline step. Because not all features are processed, the remainders are passed through instead of being dropped (default behaviour).\n",
    "                oneHotEncoder = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                            ('discretize', KBinsDiscretizer(), ['age']),\n",
    "                            ('oneHotEncoder', OneHotEncoder(handle_unknown='ignore'), lambda X : [value for value in one_hot_encoded_features if value in X.columns]),\n",
    "                        ], remainder='passthrough')\n",
    "                #build the pipeline\n",
    "                pipeline = Pipeline(steps=[\n",
    "                    ('fix_encoding_errors', FixCommonEncodingErrors()),\n",
    "                    ('transform_smoke', DataframeSmokeTransformer()),\n",
    "                    ('enrich_heart_rate', EnrichHeartData()),\n",
    "                    ('drop_columns', DropColumnsBasedOnMinimumPercentageToBeDropped()),\n",
    "                    ('oneHotEncoder', oneHotEncoder),\n",
    "                    ('impute', imputer.get('imputer')),\n",
    "                    ('scaler', scaler.get('scaler')),\n",
    "                    ('sampler', sampler),\n",
    "                    ('estimator', estimator.get(\"estimator\"))\n",
    "                ])\n",
    "                # create the inner grid search instance. No need for random_state because for integer values for cv stratified k-fold is used with shuffle=False(see:https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV parameter cv)\n",
    "                grid_search_estimator = GridSearchCV(pipeline, parameters, scoring='roc_auc', cv=10, error_score='raise', n_jobs=-1, verbose= 0)\n",
    "                # if a configuration fails it is skipped and a comment is placed in the output\n",
    "                try:\n",
    "                    print(f\"GridSearch for {scaler.get('scaler').__class__.__name__}, {estimator.get('estimator').__class__.__name__},{estimator.get('imputer').__class__.__name__} and {sampler.__class__.__name__}\")\n",
    "                    startTime = time.time()\n",
    "                    # Start nested cross validation. No need for random_state because for integer values for cv stratified k-fold is used with shuffle=False(see:https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html parameter cv)\n",
    "                    auc_best = cross_val_score(grid_search_estimator, X, y, cv=10, scoring=make_scorer(classification_report_with_auc_score), error_score='raise', verbose = 2, n_jobs=1)\n",
    "                    # determine best hyperparameters\n",
    "                    grid_search_estimator.fit(X, y)\n",
    "                    executionTime = (time.time() - startTime)\n",
    "                except Exception as e:\n",
    "                    # add comment for failed execution to output_dict\n",
    "                    print(f\"Skipping the combination of {scaler.get('scaler').__class__.__name__}, {estimator.get('estimator').__class__.__name__},{estimator.get('imputer').__class__.__name__} and {sampler.__class__.__name__} because:\")\n",
    "                    print(str(e))\n",
    "                    output_dict = {}\n",
    "                    output_dict[\"scaler\"]= scaler.get('scaler').__class__.__name__\n",
    "                    output_dict[\"estimator\"] = estimator.get('estimator').__class__.__name__\n",
    "                    output_dict[\"imputer\"] = imputer.get('imputer').__class__.__name__\n",
    "                    output_dict[\"sampler\"] = sampler.__class__.__name__\n",
    "                    output_dict[\"reason\"] = str(e)\n",
    "                else:\n",
    "                    # execution was successful. Print results and best configuration\n",
    "                    print(f\"auc for {scaler.get('scaler').__class__.__name__}, {estimator.get('estimator').__class__.__name__},{estimator.get('imputer').__class__.__name__} and {sampler.__class__.__name__} = {auc_best.mean() * 100.0}\")\n",
    "                    display(grid_search_estimator.best_params_)\n",
    "                    # create output_dict\n",
    "                    output_dict = {}\n",
    "                    output_dict[\"scaler\"]= scaler.get('scaler').__class__.__name__\n",
    "                    output_dict[\"estimator\"] = estimator.get('estimator').__class__.__name__\n",
    "                    output_dict[\"imputer\"] = imputer.get('imputer').__class__.__name__\n",
    "                    output_dict[\"sampler\"] = sampler.__class__.__name__\n",
    "                    output_dict[\"X_shape\"] = X.shape\n",
    "                    output_dict[\"one_hot_encoded_features\"] = one_hot_encoded_features\n",
    "                    output_dict[\"parameters\"] = parameters\n",
    "                    output_dict[\"auc_mean\"] = auc_best.mean() * 100\n",
    "                    output_dict[\"execution_time_in_seconds\"] = executionTime\n",
    "                    output_dict[\"best_params\"] = grid_search_estimator.best_params_\n",
    "                    # read results of outer loop from cv from pickle and add to output dictionary\n",
    "                    with open(\"temp.pickle\", \"rb\") as temp_file:\n",
    "                        report = pickle.load(temp_file)\n",
    "                        output_dict[\"auc\"] = report['auc']\n",
    "                        output_dict[\"classification_report\"] = report['classification_report']\n",
    "                        output_dict[\"confusion_matrix\"] = report['confusion_matrix']\n",
    "                finally:\n",
    "                    # read measurements from output.json if it exists, otherwise create empty list\n",
    "                    if os.path.exists('output.json'):\n",
    "                        with open(\"output.json\", \"r\") as file:\n",
    "                            file_dict = json.load(file)\n",
    "                            measurements  = file_dict.get('measurements')\n",
    "                    else:\n",
    "                        measurements = []\n",
    "                    # append new measurements to array\n",
    "                    measurements.append(output_dict)\n",
    "                    # write to output.json using CustomEncoder\n",
    "                    with open(\"output.json\", \"w\") as file:\n",
    "                        json.dump({\"measurements\": measurements}, file, cls= CustomEncoder)\n",
    "                    # remove temp.pickle if it exists\n",
    "                    if os.path.exists('temp.pickle'):\n",
    "                        os.remove('temp.pickle')\n",
    "\n",
    "\n",
    "\n",
    "        print(\"-----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/home/finn/.local/lib/python3.10/site-packages/sklearn/preprocessing/_discretization.py:291: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Negative values in data passed to CategoricalNB (input X)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [111], line 24\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#build the pipeline\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     14\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfix_encoding_errors\u001b[39m\u001b[38;5;124m'\u001b[39m, FixCommonEncodingErrors()),\n\u001b[1;32m     15\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform_smoke\u001b[39m\u001b[38;5;124m'\u001b[39m, DataframeSmokeTransformer()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m'\u001b[39m, estimator\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     23\u001b[0m ])\n\u001b[0;32m---> 24\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X,y)\n",
      "File \u001b[0;32m~/miniconda3/envs/uniMannheim-DataMining-tensorflow/lib/python3.10/site-packages/imblearn/pipeline.py:272\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    271\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 272\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:1290\u001b[0m, in \u001b[0;36mCategoricalNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1290\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:699\u001b[0m, in \u001b[0;36m_BaseDiscreteNB.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;124;03m\"\"\"Fit Naive Bayes classifier according to X, y.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     _, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    702\u001b[0m     labelbin \u001b[38;5;241m=\u001b[39m LabelBinarizer()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/naive_bayes.py:1351\u001b[0m, in \u001b[0;36mCategoricalNB._check_X_y\u001b[0;34m(self, X, y, reset)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_X_y\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1348\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1349\u001b[0m         X, y, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reset\u001b[38;5;241m=\u001b[39mreset\n\u001b[1;32m   1350\u001b[0m     )\n\u001b[0;32m-> 1351\u001b[0m     \u001b[43mcheck_non_negative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCategoricalNB (input X)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1372\u001b[0m, in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     X_min \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mmin()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_min \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNegative values in data passed to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m whom)\n",
      "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to CategoricalNB (input X)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f335c2cb84fafae796dd7ed83c640fd2cf2129dda2b9a14a13240a0604884ad1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
