{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables for configuration\n",
    "generate_pandas_profiling_reports = False\n",
    "process_preprocessed_data_of_uci = False\n",
    "print_pair_plots = False\n",
    "drop_correlated_features = False\n",
    "drop_nan= True\n",
    "encode_labels = True\n",
    "minimumPercentageMissingToBeDropped = 12\n",
    "oneHotEncodedFeatures = ['cp','restecg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib as mpl\n",
    "mpl.rc('image', cmap='coolwarm')\n",
    "\n",
    "# set pandas to show all columns of the df when using the display function\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the datasets that should be used in the current run\n",
    "datasets = [\"hungarian\", \"cleveland\", \"switzerland\", \"long-beach-va\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom function to read the dataset into a csv formated string\n",
    "# the name is used as a delimiter here because it is the last feature and common among all entrys \n",
    "def readRawData(filePath:str):\n",
    "    with open(filePath) as file:\n",
    "        dataString = file.read()\n",
    "        dataString = dataString.replace(\"\\n\",\" \")\n",
    "        dataString = re.sub(\"[a-zA-Z]+ \",\"name\\n\", dataString)\n",
    "        dataString = dataString.replace(\" \",\",\")\n",
    "        return dataString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the specified datasets into the df \n",
    "from io import StringIO\n",
    "df = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    dataset_df = pd.read_csv(StringIO(readRawData(\"./Data/\"+ dataset +\".data\")), header=None, sep=\",\")\n",
    "    dataset_df['dataset'] = dataset\n",
    "    df = pd.concat([df,dataset_df ], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=[\"id\", \"ccf\", \"age\", \"sex\", \"painloc\", \"painexer\" , \"relrest\" , \"pncaden\" , \"cp\", \"trestbps\", \"htn\", \"chol\", \"smoke\", \"cigs\", \"years\", \"fbs\", \"dm\", \"famhist\", \"restecg\", \"ekgmo\", \"ekgday\", \"ekgyr\", \"dig\", \"prop\", \"nitr\", \"pro\", \"diuretic\", \"proto\", \"thaldur\", \"thaltime\", \"met\", \"thalach\", \"thalrest\", \"tpeakbps\", \"tpeakbpd\", \"dummy\", \"trestbpd\", \"exang\", \"xhypo\", \"oldpeak\", \"slope\", \"rldv5\", \"rldv5e\", \"ca\", \"restckm\", \"exerckm\", \"restef\", \"restwm\", \"exeref\", \"exerwm\", \"thal\", \"thalsev\", \"thalpul\", \"earlobe\", \"cmo\", \"cday\", \"cyr\", \"num\", \"lmt\", \"ladprox\", \"laddist\", \"diag\", \"cxmain\", \"ramus\", \"om1\", \"om2\", \"rcaprox\", \"rcadist\", \"lvx1\", \"lvx2\", \"lvx3\", \"lvx4\", \"lvf\", \"cathef\", \"junk\", \"name\", \"dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the data from the specified datasets into the df\n",
    "from io import StringIO\n",
    "dfNew = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    dataset_df = pd.read_csv(StringIO(readRawData(\"./Data/\"+ 'new' +\".data\")), header=None, sep=\",\")\n",
    "    dataset_df['dataset'] = dataset\n",
    "    dfNew = pd.concat([dfNew,dataset_df ], ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.replace(-9, float('nan'))\n",
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the dataset encodes unfilled cells with -9 they are replaced with NaN for better compatibility with pd\n",
    "dfNew = dfNew.replace(-9, float('nan'))\n",
    "dfNew.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data cleanup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if generate_pandas_profiling_reports:\n",
    "    from pandas_profiling import ProfileReport\n",
    "    profile = ProfileReport(df, title='Pandas Profiling Report for all features')\n",
    "    profile.to_file(\"Pandas Profiling Report for all features.html\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns smoke and years both describe whether a respondent smokes or not. Smoke does this by being binary coded, while years describes the number of years a person has smoked. Due to the high number of missing values, the columns are useless on their own. However, it is possible to enrich the smoke column with the years column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of NaNs in smoke: {df['smoke'].isna().sum()}\")\n",
    "df.loc[(df['smoke'].isna()) & (df['years'] == 0),'smoke'] = 0\n",
    "df.loc[(df['smoke'].isna()) & (df['years'] > 0),'smoke'] = 1\n",
    "print(f\"Number of NaNs in smoke after combination with years: {df['smoke'].isna().sum()}\")\n",
    "df.loc[(df['smoke'].isna()) & (df['cigs'] == 0),'smoke'] = 0\n",
    "df.loc[(df['smoke'].isna()) & (df['cigs'] > 0),'smoke'] = 1\n",
    "print(f\"Number of NaNs in smoke after combination with years and cigs: {df['smoke'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding: reduces the number of missing values in smoke by 280 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if cholesterin is 0 it was not measured\n",
    "df['chol'] = df['chol'].replace(0, float('nan'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Explore how many NaNs and zeros are within one column for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.loc[ : , df.columns != 'dataset'].isna()).join(df['dataset']).groupby(\"dataset\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df.loc[ : , df.columns != 'dataset'].eq(0)).join(df['dataset']).groupby(\"dataset\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Treatment of different scales in the datasets\n",
    "### met"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot to show the reason why we need to process this data:\n",
    "sns.boxplot(x=\"met\",y=\"dataset\",data= df)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.loc[df[\"dataset\"] == \"switzerland\", \"met\"] = df.loc[df[\"dataset\"] == \"switzerland\", \"met\"]/10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot to show the reason why we need to process this data:\n",
    "sns.boxplot(x=\"met\",y=\"dataset\",data= df)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### rldv5e"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to show the reason why we need to process this data: \n",
    "sns.boxplot(x=\"rldv5e\",y=\"dataset\",data= df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"dataset\"] == \"cleveland\", \"rldv5e\"] = df.loc[df[\"dataset\"] == \"cleveland\", \"rldv5e\"]/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to show the reason why we drop:\n",
    "sns.boxplot(x=\"rldv5e\",y=\"dataset\",data= df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,15))\n",
    "# # Compute the correlation matrix\n",
    "# corr = df.corr()\n",
    "# corr = corr.round(2)\n",
    "# # Draw the heatmap with the mask and correct aspect ratio\n",
    "# sns.heatmap(corr, cmap=\"coolwarm\", center=0, square=True, linewidths=.5, vmin=-1, vmax=1, annot=True)\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# for dataset in datasets:\n",
    "#     plt.figure(figsize=(15, 15))\n",
    "#     # Compute the correlation matrix\n",
    "#     corr = df[df['dataset'] == dataset].corr()\n",
    "#     corr = corr.round(2)\n",
    "#     # Draw the heatmap with the mask and correct aspect ratio\n",
    "#     sns.heatmap(corr, cmap=\"coolwarm\", center=0, square=True, linewidths=.5, vmin=-1, vmax=1, annot=True)\n",
    "#     plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if drop_correlated_features:\n",
    "    df.drop(\"met\", inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Drop columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "irrelevant_columns = [\n",
    "    \"id\", # A id is not relevant for a model\n",
    "    \"ccf\", # the social security number does not influence if you have a heart disease or not\n",
    "    \"pncaden\", # sum of painlox painexer relrest -> the features are already in the dataset -> drop because it is a duplicate\n",
    "    \"ekgmo\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"ekgday\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"ekgyr\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"cmo\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"cday\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"cyr\", # The date of the medical examination is irrelevant for the occurrence of a disease.\n",
    "    \"name\" # Constant\n",
    "]\n",
    "df.drop(irrelevant_columns, inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unexplained_columns = [\n",
    "    \"restckm\", # irrelevant according to the uci\n",
    "    \"exerckm\", # irrelevant according to the uci\n",
    "    \"thalsev\", # irrelevant according to the uci\n",
    "    \"thalpul\", # irrelevant according to the uci\n",
    "    \"earlobe\", # Constant\n",
    "    \"lvx1\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"lvx2\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"lvx3\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"lvx4\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"lvf\", # it is not possible to gain information about what this feature measures -> could not be supplied to trained models -> drop https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "    \"dummy\", # no description available -> from the name does not seem relevant\n",
    "]\n",
    "df.drop(unexplained_columns, inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "percentage_missing = ((df.isna().sum()/len(df)*100).round(2)).to_dict()\n",
    "missing_vlaues = {key: val for key, val in percentage_missing.items() if val > minimumPercentageMissingToBeDropped}\n",
    "df.drop([*missing_vlaues.keys()], inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['dataset'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## drop by rows because of unrealistic values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave the dead ones behind\n",
    "# drop entries with a blood pressure of 0\n",
    "print(f\"Shape before drop of entrys with a blood preasure of 0: {df.shape}\")\n",
    "df.drop(df[df['trestbps'] == 0].index, inplace=True, axis=0)\n",
    "print(f\"Shape after drop of entrys with a blood preasure of 0: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop entries with unrealistic values for prop\n",
    "print(f\"Shape before drop of entries with unrealisic prop values: {df.shape}\")\n",
    "df.drop(df[df['prop'] > 1].index, inplace=True, axis=0)\n",
    "print(f\"Shape after drop of entries with unrealisic prop values: {df.shape}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# drop more columns because switzerland would be lost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# what would happen\n",
    "df.dropna(axis=0, how='any').loc[:,\"dataset\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# because of which features\n",
    "(df.loc[ : , df.columns != 'dataset'].isna()).join(df['dataset']).groupby(\"dataset\").sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df.drop([\"fbs\", \"rldv5e\", \"htn\"], inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# what would happen now\n",
    "df.dropna(axis=0, how='any').loc[:,\"dataset\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## summary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_pandas_profiling_reports:\n",
    "    profile = ProfileReport(df, title='Pandas Profiling Report for selected features')\n",
    "    profile.to_file(\"Pandas Profiling Report for selected features.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_nan:\n",
    "    # drop all entries which contain one or more NanN vlaues\n",
    "    print(f\"Shape before drop of NaN containing rows: {df.shape}\")\n",
    "    df.dropna(inplace=True, axis=0, how='any')\n",
    "    print(f\"Shape after drop of NaN containing rows: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['dataset'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Visualize Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if print_pair_plots:\n",
    "    sns.pairplot(df, hue=\"num\", palette=\"tab10\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if print_pair_plots:\n",
    "    sns.pairplot(df, hue=\"dataset\", palette=\"tab10\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# drop all rows where the label column is nan\n",
    "df = df[df['num'].notna()]\n",
    "if encode_labels:\n",
    "    labelEncoder = LabelEncoder()\n",
    "    df.loc[df['num'] >= 1,\"num\"] = 1\n",
    "    df['num'] = labelEncoder.fit_transform(df['num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if print_pair_plots:\n",
    "    sns.pairplot(df, hue=\"num\", palette=\"tab10\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the different models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from  sklearn.naive_bayes import *\n",
    "\n",
    "estimators_and_hyperparameters=[\n",
    "    {\"estimator\": CatBoostClassifier(random_state=42, thread_count=-1, silent= True), \"parameters\": {'classification__depth':[None] + [*range(1,200)],\n",
    "                                                                                                     'classification__n_estimators':range(10,1000, 100),\n",
    "                                                                                                     'classification__learning_rate':[0.001,0.01,0.1,0.2,0.3],\n",
    "                                                                                                     # 'classification__l2_leaf_reg':range(5,100, 5),\n",
    "                                                                                                     # 'classification__border_count':range(5,200, 5),\n",
    "                                                                                                     # 'classification__ctr_border_count':range(5,200, 5)\n",
    "                                                                                                     }},\n",
    "    # {\"estimator\": XGBClassifier(random_state=42, n_jobs=1), \"parameters\": {'classification__max_depth': [None] + [*range(1,200)],\n",
    "    #                                                                        'classification__n_estimators': range(10,1000, 100),\n",
    "    #                                                                         'classification__learning_rate':[0.001,0.01,0.1,0.2,0.3]}},\n",
    "    # {\"estimator\": SVC(random_state=42, tol=0.01), \"parameters\": {'classification__C': [110,120,130,140,150],\n",
    "    #                                                              'classification__gamma': [0.0001, 0.001, 0.01, 0.1],\n",
    "    #                                                              'classification__degree': [3,4,5,6],\n",
    "    #                                                              'classification__kernel':['linear', 'rbf', 'poly', 'sigmoid'] }}, # '\n",
    "    # {\"estimator\": BernoulliNB(), \"parameters\": {'classification__alpha' : np.arange(0,20,0.001)}},\n",
    "    # {\"estimator\": CategoricalNB(), \"parameters\": {'classification__alpha' : np.arange(0,20,0.001)}},\n",
    "    # {\"estimator\": ComplementNB(), \"parameters\": {'classification__alpha' : np.arange(0,20,0.001),\n",
    "    #                                              'classification__norm':[True,False]}},\n",
    "    # {\"estimator\": GaussianNB(), \"parameters\": {}},\n",
    "    # {\"estimator\": MultinomialNB(), \"parameters\": {'classification__alpha' : np.arange(0,20,0.001)}},\n",
    "    # {\"estimator\": DecisionTreeClassifier(random_state=42), \"parameters\": {'classification__criterion':['gini','entropy', 'log_loss'],\n",
    "    #                                                                       'classification__max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150],\n",
    "    #                                                                       'classification__min_samples_split': range(2,20),\n",
    "    #                                                                       'classification__min_samples_leaf': range(2,20)}},\n",
    "    # {\"estimator\": KNeighborsClassifier(), \"parameters\": {'classification__n_neighbors': range(2, 100),\n",
    "    #                                                      'classification__weights': ['uniform','distance'],\n",
    "    #                                                      'classification__p': [1,2]}},\n",
    "    {\"estimator\": RandomForestClassifier(random_state=42, n_jobs=-1), \"parameters\": {'classification__n_estimators':range(10,1000, 100),\n",
    "                                                                                     'classification__max_depth':[None] + [*range(1,200)],\n",
    "                                                                                     'classification__min_samples_split':range(2,20),\n",
    "                                                                                     'classification__min_samples_leaf': range(2,20),}},\n",
    "    # {\"estimator\": SGDClassifier(max_iter=1000000), \"parameters\": {'classification__loss':['log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    #                                               'classification__penalty':['l1','l2','elasticnet'],\n",
    "    #                                               'classification__alpha' : np.arange(1,40,1)}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *\n",
    "scalers = [\n",
    "    {\"scaler\": MaxAbsScaler(), \"parameters\": {}},\n",
    "    {\"scaler\": MinMaxScaler(), \"parameters\": {}},\n",
    "    {\"scaler\": Normalizer(), \"parameters\": {}},\n",
    "    {\"scaler\": PowerTransformer(), \"parameters\": {}},\n",
    "    {\"scaler\": RobustScaler(), \"parameters\": {}},\n",
    "    {\"scaler\": FunctionTransformer(lambda x: x), \"parameters\": {}},\n",
    "    {\"scaler\": StandardScaler(), \"parameters\": {'preprocessing__scaler__with_mean': [ True, False],'preprocessing__scaler__with_std': [ True, False]}}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X = df.loc[:,(df.columns!= 'num') & (df.columns != 'dataset') ]\n",
    "y = df.loc[: , (df.columns== 'num')].values.ravel()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# assumption the dictionarys are of equal structure\n",
    "def merge_dict(dict1, dict2):\n",
    "    for key, val in dict1.items():\n",
    "        if type(val) == dict:\n",
    "                merge_dict(dict1[key], dict2[key])\n",
    "        elif(type(val) == list):\n",
    "            dict1[key] += (';' + str(dict2[key]))\n",
    "        else:\n",
    "            dict1[key] = str(val) + ';'+ str(dict2[key])\n",
    "\n",
    "    return dict1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "\n",
    "\n",
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    confusion_matrix_dict = {}\n",
    "    for idxRow, row in np.ndenumerate(confusion_matrix(y_true, y_pred)):\n",
    "        confusion_matrix_dict[idxRow] = row\n",
    "    current_auc_score = roc_auc_score(y_true, y_pred)\n",
    "    if os.path.exists('temp.pickle'):\n",
    "        with open(\"temp.pickle\", \"rb\") as tempFile:\n",
    "            report = pickle.load(tempFile)\n",
    "            report['classification_report'] = merge_dict(report['classification_report'], classification_report(y_true, y_pred, output_dict=True))\n",
    "            report[\"auc\"].append(current_auc_score)\n",
    "            report['confusion_matrix'] = merge_dict(report['confusion_matrix'], confusion_matrix_dict)\n",
    "    else:\n",
    "        report = {'classification_report': classification_report(y_true, y_pred, output_dict=True)}\n",
    "        report[\"auc\"] = [current_auc_score]\n",
    "        report['confusion_matrix'] = confusion_matrix_dict\n",
    "    with open('temp.pickle', 'wb') as tempFile:\n",
    "        pickle.dump(report, tempFile, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    # because we need to return something\n",
    "    return current_auc_score # return accuracy score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# build pipelines\n",
    "# create the pipeline\n",
    "pipelines = []\n",
    "\n",
    "for scaler in scalers:\n",
    "    print(f\"Scaler: {scaler.get('scaler').__class__.__name__}\")\n",
    "    for estimator in estimators_and_hyperparameters:\n",
    "        startTime = time.time()\n",
    "\n",
    "        parameters = scaler.get(\"parameters\") | estimator.get(\"parameters\")\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                # use StandardScaler for Temperature and Humidity\n",
    "                ('scaler', scaler.get(\"scaler\"), X.columns),\n",
    "                # use OneHotEncoder for Outlook and Wind\n",
    "                ('encoder', OneHotEncoder(), oneHotEncodedFeatures)\n",
    "            ])\n",
    "        pipeline = Pipeline(steps=[ ('preprocessing', preprocessor), ('classification', estimator.get(\"estimator\")) ])\n",
    "        # create the grid search instance\n",
    "        grid_search_estimator = GridSearchCV(pipeline, parameters, scoring='roc_auc', cv=10, error_score='raise', n_jobs=1, verbose= 0)\n",
    "        try:\n",
    "            with open(\"output.csv\", \"a\") as file:\n",
    "                accuracy_best = cross_val_score(grid_search_estimator, X, y, cv=10, scoring=make_scorer(classification_report_with_accuracy_score), error_score='raise', verbose = 2, n_jobs=1)\n",
    "                grid_search_estimator.fit(X, y)\n",
    "                print(f\"AUC for {estimator.get('estimator').__class__.__name__} = {accuracy_best.mean() * 100.0}\")\n",
    "                display(grid_search_estimator.best_params_)\n",
    "                executionTime = (time.time() - startTime)\n",
    "\n",
    "                file.write(\"\\n\\n\")\n",
    "                file.write(f\"scaler:, {scaler.get('scaler').__class__.__name__}\\n\")\n",
    "                file.write(f\"estimator:, {estimator.get('estimator').__class__.__name__} \\n\")\n",
    "                file.write(f\"generate_pandas_profiling_reports:, {generate_pandas_profiling_reports} \\n\")\n",
    "                file.write(f\"process_preprocessed_data_of_uci:, {process_preprocessed_data_of_uci} \\n\")\n",
    "                file.write(f\"print_pair_plots:, {print_pair_plots} \\n\")\n",
    "                file.write(f\"drop_correlated_features:, {drop_correlated_features} \\n\")\n",
    "                file.write(f\"drop_nan:, {drop_nan} \\n\")\n",
    "                file.write(f\"encode_labels:, {encode_labels} \\n\")\n",
    "                file.write(f\"minimumPercentageMissingToBeDropped:, {minimumPercentageMissingToBeDropped} \\n\")\n",
    "                file.write(f\"oneHotEncodedFeatures:, {str(oneHotEncodedFeatures).replace(', ','; ')} \\n\")\n",
    "                file.write(f\"parameters:, {str(parameters).replace(', ','; ')} \\n\")\n",
    "                file.write('Execution time in seconds:, ' + str(executionTime) +'\\n')\n",
    "                file.write('AUC mean:, '+ str(accuracy_best.mean() * 100 ) +'\\n')\n",
    "                file.write(f\"best_params:, {str(grid_search_estimator.best_params_).replace(', ','; ')} \\n\")\n",
    "                with open(\"temp.pickle\", \"rb\") as tempFile:\n",
    "                    report = pickle.load(tempFile)\n",
    "\n",
    "                    file.write('AUC:,' + str(report['auc']).replace(', ', ';') +'\\n')\n",
    "                    file.write(pd.DataFrame(report['classification_report']).to_csv())\n",
    "                    file.write(pd.DataFrame(np.array([[str(report['confusion_matrix'][(0, 0)]), str(report['confusion_matrix'][(0, 1)])], [str(report['confusion_matrix'][(1, 0)]), str(report['confusion_matrix'][(1, 1)])]])).to_csv())\n",
    "                    file.write(\"\\n\\n\")\n",
    "                os.remove('temp.pickle')\n",
    "        except Exception as e:\n",
    "            print(f'Skipping the combination of {scaler.get(\"scaler\").__class__.__name__} and {estimator.get(\"estimator\").__class__.__name__} because:')\n",
    "            print(str(e))\n",
    "            with open(\"output.csv\", \"a\") as file:\n",
    "                file.write(\"\\n\\n\")\n",
    "                file.write(f'Skipping the combination of {scaler.get(\"scaler\").__class__.__name__} and {estimator.get(\"estimator\").__class__.__name__} because:')\n",
    "                file.write(str(e))\n",
    "                file.write(\"\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "    print(\"-----------------------------------------------------------------\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not process_preprocessed_data_of_uci:\n",
    "    raise SystemExit(\"So Feierabend Emma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests with the preprocessed data by the UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "datasets = [\"hungarian\", \"cleveland\", \"switzerland\", \"va\"]\n",
    "df_processed = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    dataset_df = pd.read_csv(\"./Data/processed.\"+ dataset +\".data\", header=None, sep=\",\")\n",
    "    dataset_df['dataset'] = dataset\n",
    "    df_processed = pd.concat([df_processed,dataset_df ], ignore_index=True)\n",
    "df_processed.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num', 'dataset']\n",
    "df_processed = df_processed.replace('?', float('nan'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed[['trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']] = df_processed[['trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_processed.loc[ : , df_processed.columns != 'dataset'].isna()).join(df_processed['dataset']).groupby(\"dataset\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.drop([\"slope\", \"ca\",\"thal\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape before drop of NaN containing rows: {df_processed.shape}\")\n",
    "df_processed.dropna(inplace=True, axis=0, how='any')\n",
    "print(f\"Shape after drop of NaN containing rows: {df_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_pandas_profiling_reports:\n",
    "    profile = ProfileReport(df_processed, title='Pandas Profiling Report for the features processed by the UCI')\n",
    "    profile.to_file(\"Pandas Profiling Report for the features processed by the UCI.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "df_processed.loc[df_processed['num'] >= 1,\"num\"] = 1\n",
    "df_processed['num'] = labelEncoder.fit_transform(df_processed['num'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df_processed.loc[:,(df_processed.columns!= 'num') & (df_processed.columns != 'dataset')]\n",
    "y = df_processed['num']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from statistics import mean\n",
    "for scaler in scalers:\n",
    "    print(f'Current Sclaer: {scaler.__class__.__name__}')\n",
    "    for estimator in estimators_and_hyperparameters:\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        try:\n",
    "            X_trans = scaler.fit_transform(X)\n",
    "            scores = cross_val_score(estimator[0], X_trans, y, scoring='f1',cv=skf, n_jobs=-1)\n",
    "            print(f'F1 score for {estimator[0].__class__.__name__}: {mean(scores)}')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Skipping the combination of {scaler.__class__.__name__} and {estimator[0].__class__.__name__}')\n",
    "    print('-----------------------------------------------------------------')\n",
    "print(f'Current Sclaer: NoScaler')\n",
    "for estimator in estimators_and_hyperparameters:\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    try:\n",
    "        X_trans = X\n",
    "        scores = cross_val_score(estimator[0], X_trans, y, scoring='f1',cv=skf, n_jobs=-1)\n",
    "        print(f'F1 score for {estimator[0].__class__.__name__}: {mean(scores)}')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'Skipping the combination of NoScaler and {estimator[0].__class__.__name__}')\n",
    "print('-----------------------------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests with the preprocessed data by the UCI includeing the reprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "datasets = [\"hungarian\", \"cleveland\", \"switzerland\", \"va\"]\n",
    "df_processed = pd.DataFrame()\n",
    "for dataset in datasets:\n",
    "    if dataset != \"hungarian\":\n",
    "        dataset_df = pd.read_csv(\"./Data/processed.\"+ dataset +\".data\", header=None, sep=\",\")\n",
    "        dataset_df['dataset'] = dataset\n",
    "        df_processed = pd.concat([df_processed,dataset_df ], ignore_index=True)\n",
    "with open(\"Data/reprocessed.hungarian.data\") as file:\n",
    "    dataString = file.read()\n",
    "    dataString = dataString.replace(\" \",\",\")\n",
    "    dataset_df = pd.read_csv(StringIO(dataString), header=None, sep=\",\")\n",
    "    dataset_df['dataset'] = dataset\n",
    "df_processed = pd.concat([df_processed,dataset_df ], ignore_index=True)\n",
    "df_processed.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num', 'dataset']\n",
    "df_processed = df_processed.replace('?', float('nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed[['trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']] = df_processed[['trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal']].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_pandas_profiling_reports:\n",
    "    profile = ProfileReport(df_processed, title='Pandas Profiling Report for the features processed by the UCI + reprocessed hungarian')\n",
    "    profile.to_file(\"Pandas Profiling Report for the features processed by the UCI + reprocessed hungarian.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ('dataMiningProject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": ""
  },
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
