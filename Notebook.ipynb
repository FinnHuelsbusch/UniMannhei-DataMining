{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "# ! pip install pandas pandas-profiling catboost seaborn xgboost scikit-learn nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[2K     \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.8/12.1 MB\u001B[0m \u001B[31m692.0 kB/s\u001B[0m eta \u001B[36m0:00:17\u001B[0m"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "# from pandas_profiling import ProfileReport\n",
    "import os\n",
    "import email\n",
    "import email.policy\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import catboost\n",
    "import xgboost\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/finn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download stopwords at the beginning for later usage\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Data/'\n",
    "# list of loaded emails\n",
    "mails = []\n",
    "# list of all available labels\n",
    "labels = ['easy_ham','hard_ham', 'spam', 'spam_2']\n",
    "# list of labels for the loaded emails\n",
    "content_labels = []\n",
    "\n",
    "# iterate over the datastructure by combining path with the labels\n",
    "for label in labels:\n",
    "    filenames = os.listdir(path + label +'/')\n",
    "    for file in filenames:\n",
    "        f = open((path + label + '/' + file), 'r', encoding = 'latin1')\n",
    "        content = f.read()\n",
    "        mails.append(content)\n",
    "        content_labels.append(label)\n",
    "\n",
    "# create the dataDframe from the list of mails and labels\n",
    "df = pd.DataFrame({'emails': mails, 'label': content_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "Get a rough overview over the loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Spam Data: 1002\n",
      "Size of Spam 2 Data: 1398\n",
      "Size of easy Ham Data: 5052\n",
      "Size of hard Ham Data: 501\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of Spam Data: {len(os.listdir('./Data/spam/'))}\")\n",
    "print(f\"Size of Spam 2 Data: {len(os.listdir('./Data/spam_2/'))}\")\n",
    "print(f\"Size of easy Ham Data: {len(os.listdir('./Data/easy_ham/'))}\")\n",
    "print(f\"Size of hard Ham Data: {len(os.listdir('./Data/hard_ham/'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ProfileReport(df, title=\"Pandas Profiling Report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "### Remove Duplicates\n",
    "Instead of removing all duplicates at once the duplicates are removed for each label on its own. This ensures that there are no duplicates across the different segments of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mails per label in the DataFrame before removal of duplicates:\n"
     ]
    },
    {
     "data": {
      "text/plain": "          emails\nlabel           \neasy_ham    5052\nhard_ham     501\nspam        1002\nspam_2      1398",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emails</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>easy_ham</th>\n      <td>5052</td>\n    </tr>\n    <tr>\n      <th>hard_ham</th>\n      <td>501</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>1002</td>\n    </tr>\n    <tr>\n      <th>spam_2</th>\n      <td>1398</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mails per label in the DataFrame after removal of duplicates: \n"
     ]
    },
    {
     "data": {
      "text/plain": "          emails\nlabel           \neasy_ham    4911\nhard_ham     468\nspam         992\nspam_2      1398",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emails</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>easy_ham</th>\n      <td>4911</td>\n    </tr>\n    <tr>\n      <th>hard_ham</th>\n      <td>468</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>992</td>\n    </tr>\n    <tr>\n      <th>spam_2</th>\n      <td>1398</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Mails per label in the DataFrame before removal of duplicates:')\n",
    "display(df.groupby('label').count())\n",
    "for label in labels:\n",
    "    # drop duplicates for one lable at a time\n",
    "    df[df['label'] == label] = df[df['label'] == label].drop_duplicates(subset=['emails'])\n",
    "print(f'Mails per label in the DataFrame after removal of duplicates: ')\n",
    "display(df.groupby('label').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finding: There were several duplicates in the different dataset. The removal of this duplicate entries prevents that these duplicates weighted multiple times and influence the models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df after removal of duplicates: \n"
     ]
    },
    {
     "data": {
      "text/plain": "          emails\nlabel           \neasy_ham    4911\nhard_ham     468\nspam         992\nspam_2      1398",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emails</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>easy_ham</th>\n      <td>4911</td>\n    </tr>\n    <tr>\n      <th>hard_ham</th>\n      <td>468</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>992</td>\n    </tr>\n    <tr>\n      <th>spam_2</th>\n      <td>1398</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=['emails'])\n",
    "print('Shape of df after removal of duplicates: ')\n",
    "display(df.groupby('label').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding: The removal of duplicates across all labels at once show that duplicates exist only within a class, but not across class boundaries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove empty Cells\n",
    "Cells are removed if one cell in the row is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mails per label in the DataFrame before the removal of empty rows: \n"
     ]
    },
    {
     "data": {
      "text/plain": "          emails\nlabel           \neasy_ham    4911\nhard_ham     468\nspam         992\nspam_2      1398",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emails</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>easy_ham</th>\n      <td>4911</td>\n    </tr>\n    <tr>\n      <th>hard_ham</th>\n      <td>468</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>992</td>\n    </tr>\n    <tr>\n      <th>spam_2</th>\n      <td>1398</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mails per label in the DataFrame after the removal of empty rows: \n"
     ]
    },
    {
     "data": {
      "text/plain": "          emails\nlabel           \neasy_ham    4911\nhard_ham     468\nspam         992\nspam_2      1398",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>emails</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>easy_ham</th>\n      <td>4911</td>\n    </tr>\n    <tr>\n      <th>hard_ham</th>\n      <td>468</td>\n    </tr>\n    <tr>\n      <th>spam</th>\n      <td>992</td>\n    </tr>\n    <tr>\n      <th>spam_2</th>\n      <td>1398</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Mails per label in the DataFrame before the removal of empty rows: ')\n",
    "display(df.groupby('label').count())\n",
    "df.dropna(how='any', inplace = True)\n",
    "print('Mails per label in the DataFrame after the removal of empty rows: ')\n",
    "display(df.groupby('label').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding: There were no empty Rows in the DataFrame. -> no empty emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create proper labels\n",
    "Spam and not spam are the classes that should later be classified. The current labels ('easy_ham', 'hard_ham', 'spam', 'spam_2') should be preserved for later analysis. A label encoder is not used, instead the labels are assigned manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['former_label'] = df['label']\n",
    "# change labels from the old ones to spam and not spam\n",
    "df.loc[df['label'].str.contains('spam') == True,'label'] = 'spam'\n",
    "df.loc[df['label'].str.contains('ham') == True,'label'] = 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transform loaded String messages into E-Mail objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the emails into an Email Object\n",
    "df['emails'] = df['emails'].transform(lambda emails: email.message_from_string(emails, policy= email.policy.EmailPolicy(utf8=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract features from E-Mail the Object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO instead of counting how often an emailtype is present build a string that shows how the types are nested\n",
    "def extract_features_from_email(emails: email):\n",
    "    # declare and initialize a dictionary that is later used for creating the Series.\n",
    "    # The initialization already contains the inforamtion if the email is a multipart message because it is just read from the object.\n",
    "    values = {\"is_multipart\": bool(emails.is_multipart())}\n",
    "\n",
    "    # helper function to add / append a value to the values dictionary.\n",
    "    # strings are appended with a ' ' inbetween\n",
    "    def add_to_dict(key, value):\n",
    "        if key not in values:\n",
    "            values[key] = value\n",
    "        else:\n",
    "            if isinstance(value, str):\n",
    "                values[key] +=  \" \" + value\n",
    "            else:\n",
    "                values[key] += value\n",
    "\n",
    "    # walk the email messages.\n",
    "    # Multipart messages contain multiple header fields (see  RFC 2045, RFC 2046, RFC 2047, RFC 4288, RFC 4289 and RFC 2049 for further information)\n",
    "    for part in emails.walk():\n",
    "        # get the type of the current part of the email\n",
    "        message_content_type = part.get_content_type()\n",
    "        # count how often each content type is present in the email conversation by adding or incrementing the entry in the dictionary\n",
    "        add_to_dict('content_type_' + message_content_type, 1)\n",
    "        if message_content_type not in ['text/plain','text/html']:\n",
    "            # continue because it is not possible to further interpret this part of the message.\n",
    "            # if a multipart message is determined the \"children\" have been or will be visited by the walk of the e mail object\n",
    "            continue\n",
    "        try:\n",
    "            message_content = part.get_content()\n",
    "        except Exception as e:\n",
    "            # encoding error take the complete payload as body. Will overcome this issue in later steps\n",
    "            message_content = part.get_payload()\n",
    "        if message_content_type == 'text/plain':\n",
    "            add_to_dict('content', message_content)\n",
    "        else:\n",
    "            # decode the html back to plain text by bs4\n",
    "            try:\n",
    "                soup = BeautifulSoup(message_content, 'html.parser')\n",
    "                add_to_dict('content', soup.text)\n",
    "            except:\n",
    "                add_to_dict('content', \"empty\")\n",
    "    # assumption the subjects of the different messages within an conversation do not differ significant\n",
    "    add_to_dict('subject', emails['subject'])\n",
    "    return pd.Series(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "Index([], dtype='object')"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.isna().any()\n",
    "s[s].index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from the email Object and add them as new column\n",
    "# fillna can is used to fill content_types that are not present in a E-Mail but in others. In this case the fields are initialized with NaN but 0 is correct.\n",
    "df = df.join(df.apply(lambda x: extract_features_from_email(x['emails']),axis=1).fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "data": {
      "text/plain": "Index([], dtype='object')"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.isna().any()\n",
    "s[s].index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [],
   "source": [
    "# count how often each ignored character is ignored\n",
    "\n",
    "# string_of_all_ignored_characters = str([' '.join(df['content'].transform(lambda x: message_content_cleanup(str(x))).transform(lambda x: re.sub('[a-zA-Z]', '', str(x))))]).replace(' ','')\n",
    "# count = {}\n",
    "# for i in string_of_all_ignored_characters:\n",
    "#     if i in count: #check if it exists in dictionary\n",
    "#         count[i] += 1\n",
    "#     else:\n",
    "#         count[i] = 1 #first occurrence of character\n",
    "# print(f'number of ignored characters: {len(count.keys())}')\n",
    "# dict(sorted(count.items(), key=lambda item: item[1], reverse=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# for message in df[df['is_multipart'] == True].iloc[0].emails.walk():\n",
    "#     print(message['subject'])\n",
    "#     print(message.get_content_type())\n",
    "#     print(message)\n",
    "#     print('---------')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_content_cleanup(message_content: str):\n",
    "    # transform the text to lower case\n",
    "    message_content = message_content.lower()\n",
    "    # remove linebreaks and tabs\n",
    "    message_content = message_content.replace('\\t', ' ')\n",
    "    message_content = message_content.replace('\\n', ' ')\n",
    "    # separate punctuation from surrounding text\n",
    "    message_content = message_content.replace('.',' . ')\n",
    "    message_content = message_content.replace(',',' , ')\n",
    "    message_content = message_content.replace('!',' ! ')\n",
    "    message_content = message_content.replace('?',' ? ')\n",
    "    # remove double spaces that might have be introduced in the previous step\n",
    "    message_content = message_content.replace('  ',' ')\n",
    "    return message_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "# create word stemmer object for usage in the function\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def build_word_stems(message_content: str):\n",
    "    message_content = re.sub('[^a-zA-Z]', ' ', message_content)\n",
    "    message_content = message_content.split()\n",
    "    message_content = [stemmer.stem(word) for word in message_content if word not in stopwords.words('english')]\n",
    "    message_content = ' '.join(message_content)\n",
    "    return message_content"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=os.cpu_count()):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def process_content_and_subject(local_df):\n",
    "    local_df['content'] = local_df['content'].transform(lambda x: message_content_cleanup(str(x)))\n",
    "    local_df['content_stemmed'] = local_df['content'].transform(lambda x: build_word_stems(str(x)))\n",
    "    local_df['subject'] = local_df['subject'].transform(lambda x: message_content_cleanup(str(x)))\n",
    "    local_df['subject_stemmed'] = local_df['subject'].transform(lambda x: build_word_stems(str(x)))\n",
    "    return local_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [
    {
     "data": {
      "text/plain": "Index([], dtype='object')"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.isna().any()\n",
    "s[s].index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "df = parallelize_dataframe(df, process_content_and_subject)\n",
    "df_backup = deepcopy(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "Index([], dtype='object')"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.isna().any()\n",
    "s[s].index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "content_count_vectorizer = CountVectorizer(max_features=100)\n",
    "subject_count_vectorizer = CountVectorizer(max_features=2000)\n",
    "\n",
    "content_vectorized = content_count_vectorizer.fit_transform(df['content_stemmed'].to_numpy()).toarray()\n",
    "subject_vectorized = subject_count_vectorizer.fit_transform(df['subject_stemmed'].to_numpy()).toarray()\n",
    "\n",
    "X_content = pd.DataFrame(content_vectorized, columns=  [\"content_\" + x for x in content_count_vectorizer.get_feature_names_out()])\n",
    "X_subject = pd.DataFrame(subject_vectorized, columns=  [\"subject_\" + x for x in subject_count_vectorizer.get_feature_names_out()])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "Index([], dtype='object')"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.isna().any()\n",
    "s[s].index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "(7769, 2122)"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_content.join(X_subject)\n",
    "X = X.join(df.filter(regex='content_type_*'))\n",
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "(7769,)"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['label']\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from catboost import CatBoostClassifier\n",
    "from  sklearn.naive_bayes import *\n",
    "estimators_and_hyperparameters=[\n",
    "    (CatBoostClassifier(random_state=42, thread_count=-1, silent= True),{}),\n",
    "    (XGBClassifier(random_state=42, n_jobs=-1, silent= True),{}),\n",
    "    (SVC(kernel='linear',random_state=42),{}),\n",
    "    (SVC(kernel='poly',random_state=42),{}),\n",
    "    (SVC(kernel='rbf',random_state=42),{}),\n",
    "    (SVC(kernel='sigmoid',random_state=42),{}),\n",
    "    #(SVC(kernel='precomputed',random_state=42),{}),\n",
    "    (BernoulliNB(),{}),\n",
    "    #(CategoricalNB(),{}),\n",
    "    (ComplementNB(),{}),\n",
    "    (GaussianNB(),{}),\n",
    "    (MultinomialNB(),{}),\n",
    "    (DecisionTreeClassifier(random_state=42),{}),\n",
    "    (KNeighborsClassifier(n_jobs=-1),{}),\n",
    "    (RandomForestClassifier(random_state=42, n_jobs=-1), {})\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "(7769, 30)"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "(7769, 2122)"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "Index([], dtype='object')"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = X.isna().any()\n",
    "s[s].index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [],
   "source": [
    "X_new = X.dropna(how='any', inplace=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "X = X.fillna(0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "[(<catboost.core.CatBoostClassifier at 0x7f138af24c70>, {}),\n (XGBClassifier(base_score=None, booster=None, callbacks=None,\n                colsample_bylevel=None, colsample_bynode=None,\n                colsample_bytree=None, early_stopping_rounds=None,\n                enable_categorical=False, eval_metric=None, gamma=None,\n                gpu_id=None, grow_policy=None, importance_type=None,\n                interaction_constraints=None, learning_rate=None, max_bin=None,\n                max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n                max_leaves=None, min_child_weight=None, missing=nan,\n                monotone_constraints=None, n_estimators=100, n_jobs=-1,\n                num_parallel_tree=None, predictor=None, random_state=42,\n                reg_alpha=None, reg_lambda=None, ...),\n  {}),\n (SVC(kernel='linear', random_state=42), {}),\n (SVC(kernel='poly', random_state=42), {}),\n (SVC(random_state=42), {}),\n (SVC(kernel='sigmoid', random_state=42), {}),\n (BernoulliNB(), {}),\n (CategoricalNB(), {}),\n (ComplementNB(), {}),\n (GaussianNB(), {}),\n (MultinomialNB(), {}),\n (DecisionTreeClassifier(random_state=42), {}),\n (KNeighborsClassifier(n_jobs=-1), {}),\n (RandomForestClassifier(n_jobs=-1, random_state=42), {})]"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators_and_hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for CatBoostClassifier: 0.9557548717898716\n",
      "F1 score for XGBClassifier: 0.9749400898295304\n",
      "F1 score for SVC: 0.9651086442251569\n",
      "F1 score for SVC: 0.06460587982817423\n",
      "F1 score for SVC: 0.7498516573940206\n",
      "F1 score for SVC: 0.6033441782735508\n",
      "F1 score for BernoulliNB: 0.8779553629966719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 81, in predict\n",
      "    jll = self._joint_log_likelihood(X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 1425, in _joint_log_likelihood\n",
      "    jll += self.feature_log_prob_[i][:, indices].T\n",
      "IndexError: index 20 is out of bounds for axis 1 with size 19\n",
      "\n",
      "  warnings.warn(\n",
      "/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 81, in predict\n",
      "    jll = self._joint_log_likelihood(X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 1425, in _joint_log_likelihood\n",
      "    jll += self.feature_log_prob_[i][:, indices].T\n",
      "IndexError: index 812 is out of bounds for axis 1 with size 439\n",
      "\n",
      "  warnings.warn(\n",
      "/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 81, in predict\n",
      "    jll = self._joint_log_likelihood(X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 1425, in _joint_log_likelihood\n",
      "    jll += self.feature_log_prob_[i][:, indices].T\n",
      "IndexError: index 12 is out of bounds for axis 1 with size 10\n",
      "\n",
      "  warnings.warn(\n",
      "/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 81, in predict\n",
      "    jll = self._joint_log_likelihood(X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 1425, in _joint_log_likelihood\n",
      "    jll += self.feature_log_prob_[i][:, indices].T\n",
      "IndexError: index 223 is out of bounds for axis 1 with size 146\n",
      "\n",
      "  warnings.warn(\n",
      "/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:776: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 106, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 261, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 81, in predict\n",
      "    jll = self._joint_log_likelihood(X)\n",
      "  File \"/home/finn/miniconda3/envs/uniDataMining/lib/python3.10/site-packages/sklearn/naive_bayes.py\", line 1425, in _joint_log_likelihood\n",
      "    jll += self.feature_log_prob_[i][:, indices].T\n",
      "IndexError: index 2327 is out of bounds for axis 1 with size 212\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for CategoricalNB: nan\n",
      "F1 score for ComplementNB: 0.9182323784529879\n",
      "F1 score for GaussianNB: 0.8363582303273057\n",
      "F1 score for MultinomialNB: 0.912748156646767\n",
      "F1 score for DecisionTreeClassifier: 0.9300896224722159\n",
      "F1 score for KNeighborsClassifier: 0.893830418579902\n",
      "F1 score for RandomForestClassifier: 0.9733097718612462\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from statistics import mean\n",
    "X = X.reindex()\n",
    "y = y.reindex()\n",
    "\n",
    "for estimator in estimators_and_hyperparameters:\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # for train_index, test_index in skf.split(X, y):\n",
    "    #     print(train_index)\n",
    "    #     X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "    #     y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    #     estimator[0].fit(X_train_fold, y_train_fold)\n",
    "    #     scores.append(estimator[0].score(X_test_fold, y_test_fold))\n",
    "    scores = cross_val_score(estimator[0], X, y, scoring='f1',cv=skf, n_jobs=-1)\n",
    "    print(f'F1 score for {estimator[0].__class__.__name__}: {mean(scores)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sklearn.metrics.get_scorer_names()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
