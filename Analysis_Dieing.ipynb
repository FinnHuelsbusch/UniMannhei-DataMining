{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9438b12c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "# loading XGBoost\n",
    "with open(\"outputs/output xgboost.json\", 'r') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "# sort dataset by normalization\n",
    "df_XGBoost = pd.json_normalize(data, record_path =['measurements'])\n",
    "\n",
    "# save minimum percentage to be dropped in extra variable\n",
    "df_XGBoost[\"drop_columns\"] = df_XGBoost[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b338096",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "         scaler      estimator        imputer             sampler    X_shape  \\\n0  MaxAbsScaler  XGBClassifier  SimpleImputer                 str  [899, 43]   \n1  MaxAbsScaler  XGBClassifier  SimpleImputer   RandomOverSampler  [899, 43]   \n2  MaxAbsScaler  XGBClassifier  SimpleImputer  RandomUnderSampler  [899, 43]   \n3  MinMaxScaler  XGBClassifier  SimpleImputer                 str  [899, 43]   \n4  MinMaxScaler  XGBClassifier  SimpleImputer   RandomOverSampler  [899, 43]   \n\n           one_hot_encoded_features   auc_mean  execution_time_in_seconds  \\\n0  [cp, restecg, slope, ca, restwm]  75.027738                 122.561587   \n1  [cp, restecg, slope, ca, restwm]  75.438477                 123.505258   \n2  [cp, restecg, slope, ca, restwm]  75.162469                 107.387465   \n3  [cp, restecg, slope, ca, restwm]  75.050697                 116.305835   \n4  [cp, restecg, slope, ca, restwm]  75.438477                 122.199511   \n\n                                                 auc  \\\n0  [0.714534594325535, 0.7675460428073669, 0.8188...   \n1  [0.7349427575908413, 0.7797411647585863, 0.771...   \n2  [0.7553509208561474, 0.7899452463912394, 0.794...   \n3  [0.714534594325535, 0.7675460428073669, 0.8188...   \n4  [0.7349427575908413, 0.7797411647585863, 0.771...   \n\n     parameters.impute__strategy  ...   auc_std  pre_mean  rec_mean    f1_std  \\\n0  [mean, median, most_frequent]  ...  0.076513  0.768763  0.753995  0.086839   \n1  [mean, median, most_frequent]  ...  0.070096  0.775624  0.757341  0.080040   \n2  [mean, median, most_frequent]  ...  0.081176  0.765268  0.753970  0.091534   \n3  [mean, median, most_frequent]  ...  0.075209  0.768886  0.753995  0.085831   \n4  [mean, median, most_frequent]  ...  0.070096  0.775624  0.757341  0.080040   \n\n    f1_mean type2  auc_confl  auc_confu  f1_confl  f1_confu  \n0  0.744731   108  74.980315  75.075161  0.690908  0.798555  \n1  0.748220   110  75.395031  75.481923  0.698611  0.797830  \n2  0.745038   114  75.112155  75.212783  0.688304  0.801771  \n3  0.744858   109  75.004082  75.097312  0.691660  0.798057  \n4  0.748220   110  75.395031  75.481923  0.698611  0.797830  \n\n[5 rows x 55 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>scaler</th>\n      <th>estimator</th>\n      <th>imputer</th>\n      <th>sampler</th>\n      <th>X_shape</th>\n      <th>one_hot_encoded_features</th>\n      <th>auc_mean</th>\n      <th>execution_time_in_seconds</th>\n      <th>auc</th>\n      <th>parameters.impute__strategy</th>\n      <th>...</th>\n      <th>auc_std</th>\n      <th>pre_mean</th>\n      <th>rec_mean</th>\n      <th>f1_std</th>\n      <th>f1_mean</th>\n      <th>type2</th>\n      <th>auc_confl</th>\n      <th>auc_confu</th>\n      <th>f1_confl</th>\n      <th>f1_confu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MaxAbsScaler</td>\n      <td>XGBClassifier</td>\n      <td>SimpleImputer</td>\n      <td>str</td>\n      <td>[899, 43]</td>\n      <td>[cp, restecg, slope, ca, restwm]</td>\n      <td>75.027738</td>\n      <td>122.561587</td>\n      <td>[0.714534594325535, 0.7675460428073669, 0.8188...</td>\n      <td>[mean, median, most_frequent]</td>\n      <td>...</td>\n      <td>0.076513</td>\n      <td>0.768763</td>\n      <td>0.753995</td>\n      <td>0.086839</td>\n      <td>0.744731</td>\n      <td>108</td>\n      <td>74.980315</td>\n      <td>75.075161</td>\n      <td>0.690908</td>\n      <td>0.798555</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>MaxAbsScaler</td>\n      <td>XGBClassifier</td>\n      <td>SimpleImputer</td>\n      <td>RandomOverSampler</td>\n      <td>[899, 43]</td>\n      <td>[cp, restecg, slope, ca, restwm]</td>\n      <td>75.438477</td>\n      <td>123.505258</td>\n      <td>[0.7349427575908413, 0.7797411647585863, 0.771...</td>\n      <td>[mean, median, most_frequent]</td>\n      <td>...</td>\n      <td>0.070096</td>\n      <td>0.775624</td>\n      <td>0.757341</td>\n      <td>0.080040</td>\n      <td>0.748220</td>\n      <td>110</td>\n      <td>75.395031</td>\n      <td>75.481923</td>\n      <td>0.698611</td>\n      <td>0.797830</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MaxAbsScaler</td>\n      <td>XGBClassifier</td>\n      <td>SimpleImputer</td>\n      <td>RandomUnderSampler</td>\n      <td>[899, 43]</td>\n      <td>[cp, restecg, slope, ca, restwm]</td>\n      <td>75.162469</td>\n      <td>107.387465</td>\n      <td>[0.7553509208561474, 0.7899452463912394, 0.794...</td>\n      <td>[mean, median, most_frequent]</td>\n      <td>...</td>\n      <td>0.081176</td>\n      <td>0.765268</td>\n      <td>0.753970</td>\n      <td>0.091534</td>\n      <td>0.745038</td>\n      <td>114</td>\n      <td>75.112155</td>\n      <td>75.212783</td>\n      <td>0.688304</td>\n      <td>0.801771</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>MinMaxScaler</td>\n      <td>XGBClassifier</td>\n      <td>SimpleImputer</td>\n      <td>str</td>\n      <td>[899, 43]</td>\n      <td>[cp, restecg, slope, ca, restwm]</td>\n      <td>75.050697</td>\n      <td>116.305835</td>\n      <td>[0.714534594325535, 0.7675460428073669, 0.8188...</td>\n      <td>[mean, median, most_frequent]</td>\n      <td>...</td>\n      <td>0.075209</td>\n      <td>0.768886</td>\n      <td>0.753995</td>\n      <td>0.085831</td>\n      <td>0.744858</td>\n      <td>109</td>\n      <td>75.004082</td>\n      <td>75.097312</td>\n      <td>0.691660</td>\n      <td>0.798057</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>MinMaxScaler</td>\n      <td>XGBClassifier</td>\n      <td>SimpleImputer</td>\n      <td>RandomOverSampler</td>\n      <td>[899, 43]</td>\n      <td>[cp, restecg, slope, ca, restwm]</td>\n      <td>75.438477</td>\n      <td>122.199511</td>\n      <td>[0.7349427575908413, 0.7797411647585863, 0.771...</td>\n      <td>[mean, median, most_frequent]</td>\n      <td>...</td>\n      <td>0.070096</td>\n      <td>0.775624</td>\n      <td>0.757341</td>\n      <td>0.080040</td>\n      <td>0.748220</td>\n      <td>110</td>\n      <td>75.395031</td>\n      <td>75.481923</td>\n      <td>0.698611</td>\n      <td>0.797830</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 55 columns</p>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate main values of interest\n",
    "df_XGBoost[\"auc_std\"] = df_XGBoost.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "df_XGBoost[\"pre_mean\"] = df_XGBoost.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "df_XGBoost[\"rec_mean\"] = df_XGBoost.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "df_XGBoost[\"f1_std\"] = df_XGBoost.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_XGBoost[\"f1_mean\"] = df_XGBoost.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_XGBoost[\"type2\"] = df_XGBoost.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "# Confidence Intervalls\n",
    "df_XGBoost[\"auc_confl\"] = df_XGBoost[\"auc_mean\"]- 1.96 * (df_XGBoost[\"auc_std\"] / np.sqrt(10))\n",
    "df_XGBoost[\"auc_confu\"] = df_XGBoost[\"auc_mean\"]+ 1.96 * (df_XGBoost[\"auc_std\"] / np.sqrt(10))\n",
    "df_XGBoost[\"f1_confl\"] = df_XGBoost[\"f1_mean\"]- 1.96 * (df_XGBoost[\"f1_std\"] / np.sqrt(10))\n",
    "df_XGBoost[\"f1_confu\"] = df_XGBoost[\"f1_mean\"]+ 1.96 * (df_XGBoost[\"f1_std\"] / np.sqrt(10))\n",
    "# get a look of the final dataset\n",
    "df_XGBoost.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3323e831",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std  type2  drop_columns\n7   76.971665  0.074685     96           100\n8   76.540518  0.086870    109            75\n9   75.475697  0.080552    106           100\n1   75.438477  0.070096    110           100\n4   75.438477  0.070096    110           100\n10  75.438477  0.070096    110           100\n13  75.438477  0.070096    110           100\n19  75.438477  0.070096    110           100\n15  75.402738  0.075977    108           100\n18  75.352738  0.076452    106           100\n14  75.312469  0.081907    115            75\n17  75.287469  0.081928    114           100\n20  75.287469  0.081928    114           100\n5   75.287469  0.081928    114            75\n6   75.280811  0.082558     98           100\n12  75.277738  0.074727    108           100\n16  75.234395  0.075600    112           100\n2   75.162469  0.081176    114            75\n11  75.062469  0.081313    115            75\n3   75.050697  0.075209    109           100",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7</th>\n      <td>76.971665</td>\n      <td>0.074685</td>\n      <td>96</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>76.540518</td>\n      <td>0.086870</td>\n      <td>109</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>75.475697</td>\n      <td>0.080552</td>\n      <td>106</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>75.438477</td>\n      <td>0.070096</td>\n      <td>110</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>75.438477</td>\n      <td>0.070096</td>\n      <td>110</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>75.438477</td>\n      <td>0.070096</td>\n      <td>110</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>75.438477</td>\n      <td>0.070096</td>\n      <td>110</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>75.438477</td>\n      <td>0.070096</td>\n      <td>110</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>75.402738</td>\n      <td>0.075977</td>\n      <td>108</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>75.352738</td>\n      <td>0.076452</td>\n      <td>106</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>75.312469</td>\n      <td>0.081907</td>\n      <td>115</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>75.287469</td>\n      <td>0.081928</td>\n      <td>114</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>75.287469</td>\n      <td>0.081928</td>\n      <td>114</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>75.287469</td>\n      <td>0.081928</td>\n      <td>114</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>75.280811</td>\n      <td>0.082558</td>\n      <td>98</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>75.277738</td>\n      <td>0.074727</td>\n      <td>108</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>75.234395</td>\n      <td>0.075600</td>\n      <td>112</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>75.162469</td>\n      <td>0.081176</td>\n      <td>114</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>75.062469</td>\n      <td>0.081313</td>\n      <td>115</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>75.050697</td>\n      <td>0.075209</td>\n      <td>109</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only display variables important for the model evaluation\n",
    "df_XGBoost = df_XGBoost.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "df_XGBoost[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44ccfe22",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "scaler                  Normalizer\nestimator            XGBClassifier\nimputer              SimpleImputer\nsampler         RandomUnderSampler\nauc_mean                 76.540518\nauc_confl                76.486675\nauc_confu                 76.59436\ntype2                          109\nf1_mean                   0.759294\nf1_confl                  0.700291\nf1_confu                  0.818296\ndrop_columns                    75\npre_mean                  0.778677\nrec_mean                  0.767316\nName: 8, dtype: object"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model\n",
    "XGBoost = df_XGBoost.loc[8]\n",
    "XGBoost[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85cc427c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std  type2  drop_columns\n15  80.398731  0.097697     83           100\n17  79.798258  0.090229    100           100\n16  79.383997  0.092650    109           100\n5   78.938589  0.063572    127           100\n4   78.811013  0.065513    121           100\n11  78.790070  0.090879    105           100\n2   78.481906  0.065569    123           100\n9   78.180338  0.088097     95           100\n10  78.144126  0.095192    114           100\n0   77.672623  0.075865    109           100\n3   77.645582  0.089669    108            75\n12  77.281396  0.105067     94            20\n1   77.134930  0.067626    134           100\n18  76.285254  0.056878     70             0\n14  76.110988  0.096023    118            20\n8   76.010876  0.048473    107             0\n19  75.770993  0.051443     85             0\n13  75.729890  0.093557    123            20\n20  75.626643  0.056547     95             0\n7   74.909843  0.049239    108             0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>80.398731</td>\n      <td>0.097697</td>\n      <td>83</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>79.798258</td>\n      <td>0.090229</td>\n      <td>100</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>79.383997</td>\n      <td>0.092650</td>\n      <td>109</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>78.938589</td>\n      <td>0.063572</td>\n      <td>127</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>78.811013</td>\n      <td>0.065513</td>\n      <td>121</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>78.790070</td>\n      <td>0.090879</td>\n      <td>105</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>78.481906</td>\n      <td>0.065569</td>\n      <td>123</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>78.180338</td>\n      <td>0.088097</td>\n      <td>95</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>78.144126</td>\n      <td>0.095192</td>\n      <td>114</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>77.672623</td>\n      <td>0.075865</td>\n      <td>109</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>77.645582</td>\n      <td>0.089669</td>\n      <td>108</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>77.281396</td>\n      <td>0.105067</td>\n      <td>94</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>77.134930</td>\n      <td>0.067626</td>\n      <td>134</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>76.285254</td>\n      <td>0.056878</td>\n      <td>70</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>76.110988</td>\n      <td>0.096023</td>\n      <td>118</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>76.010876</td>\n      <td>0.048473</td>\n      <td>107</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>75.770993</td>\n      <td>0.051443</td>\n      <td>85</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>75.729890</td>\n      <td>0.093557</td>\n      <td>123</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>75.626643</td>\n      <td>0.056547</td>\n      <td>95</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>74.909843</td>\n      <td>0.049239</td>\n      <td>108</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading KNeighbors \n",
    "with open(\"outputs/output KNN.json\", 'r') as f:\n",
    "  data_KNeighbors = json.load(f)\n",
    "\n",
    "df_KNeighbors = pd.json_normalize(data_KNeighbors, record_path =['measurements'])\n",
    "df_KNeighbors[\"drop_columns\"] = df_KNeighbors[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "df_KNeighbors[\"auc_std\"] = df_KNeighbors.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "df_KNeighbors[\"pre_mean\"] = df_KNeighbors.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "df_KNeighbors[\"rec_mean\"] = df_KNeighbors.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "df_KNeighbors[\"f1_std\"] = df_KNeighbors.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_KNeighbors[\"f1_mean\"] = df_KNeighbors.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_KNeighbors[\"type2\"] = df_KNeighbors.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "df_KNeighbors[\"auc_confl\"] = df_KNeighbors[\"auc_mean\"]- 1.96 * (df_KNeighbors[\"auc_std\"] / np.sqrt(10))\n",
    "df_KNeighbors[\"auc_confu\"] = df_KNeighbors[\"auc_mean\"]+ 1.96 * (df_KNeighbors[\"auc_std\"] / np.sqrt(10))\n",
    "df_KNeighbors[\"f1_confl\"] = df_KNeighbors[\"f1_mean\"]- 1.96 * (df_KNeighbors[\"f1_std\"] / np.sqrt(10))\n",
    "df_KNeighbors[\"f1_confu\"] = df_KNeighbors[\"f1_mean\"]+ 1.96 * (df_KNeighbors[\"f1_std\"] / np.sqrt(10))\n",
    "df_KNeighbors.head()\n",
    "df_KNeighbors = df_KNeighbors.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "df_KNeighbors[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf9a551",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "scaler                           str\nestimator       KNeighborsClassifier\nimputer                SimpleImputer\nsampler                          str\nauc_mean                   76.285254\nauc_confl                  76.250001\nauc_confu                  76.320507\ntype2                             70\nf1_mean                     0.767047\nf1_confl                    0.732173\nf1_confu                     0.80192\ndrop_columns                       0\npre_mean                    0.781214\nrec_mean                    0.772996\nName: 18, dtype: object"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighbors = df_KNeighbors.loc[18]\n",
    "KNeighbors[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab11529e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std  type2  drop_columns\n15  78.332342  0.097413     79           100\n12  78.205301  0.104771     79           100\n3   78.190480  0.095344     78           100\n9   78.160391  0.095387     77           100\n16  78.105811  0.093092     96            75\n1   78.103771  0.094111     96            75\n0   78.065480  0.098726     78           100\n4   78.041551  0.095939     99            75\n13  77.903771  0.094006     98            75\n10  77.889983  0.106215     92            75\n8   77.758885  0.083667     97            75\n2   76.730338  0.095331    112           100\n6   76.531844  0.072818     98            75\n7   76.524664  0.097920    103            75\n11  76.464535  0.099508    111            75\n17  75.908387  0.099018    114           100\n14  75.852290  0.101550    117           100\n5   75.830338  0.101702    116           100",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>78.332342</td>\n      <td>0.097413</td>\n      <td>79</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>78.205301</td>\n      <td>0.104771</td>\n      <td>79</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>78.190480</td>\n      <td>0.095344</td>\n      <td>78</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>78.160391</td>\n      <td>0.095387</td>\n      <td>77</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>78.105811</td>\n      <td>0.093092</td>\n      <td>96</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>78.103771</td>\n      <td>0.094111</td>\n      <td>96</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>78.065480</td>\n      <td>0.098726</td>\n      <td>78</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>78.041551</td>\n      <td>0.095939</td>\n      <td>99</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>77.903771</td>\n      <td>0.094006</td>\n      <td>98</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>77.889983</td>\n      <td>0.106215</td>\n      <td>92</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>77.758885</td>\n      <td>0.083667</td>\n      <td>97</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>76.730338</td>\n      <td>0.095331</td>\n      <td>112</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>76.531844</td>\n      <td>0.072818</td>\n      <td>98</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>76.524664</td>\n      <td>0.097920</td>\n      <td>103</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>76.464535</td>\n      <td>0.099508</td>\n      <td>111</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>75.908387</td>\n      <td>0.099018</td>\n      <td>114</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>75.852290</td>\n      <td>0.101550</td>\n      <td>117</td>\n      <td>100</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>75.830338</td>\n      <td>0.101702</td>\n      <td>116</td>\n      <td>100</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading Random Forest Classifier\n",
    "with open(\"outputs/output RandomForestClassifier.json\", 'r') as f:\n",
    "  data_Forest = json.load(f)\n",
    "\n",
    "df_Forest = pd.json_normalize(data_Forest, record_path =['measurements'])\n",
    "df_Forest[\"drop_columns\"] = df_Forest[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "df_Forest[\"auc_std\"] = df_Forest.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "df_Forest[\"pre_mean\"] = df_Forest.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "df_Forest[\"rec_mean\"] = df_Forest.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "df_Forest[\"f1_std\"] = df_Forest.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_Forest[\"f1_mean\"] = df_Forest.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_Forest[\"type2\"] = df_Forest.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "df_Forest[\"auc_confl\"] = df_Forest[\"auc_mean\"]- 1.96 * (df_Forest[\"auc_std\"] / np.sqrt(10))\n",
    "df_Forest[\"auc_confu\"] = df_Forest[\"auc_mean\"]+ 1.96 * (df_Forest[\"auc_std\"] / np.sqrt(10))\n",
    "df_Forest[\"f1_confl\"] = df_Forest[\"f1_mean\"]- 1.96 * (df_Forest[\"f1_std\"] / np.sqrt(10))\n",
    "df_Forest[\"f1_confu\"] = df_Forest[\"f1_mean\"]+ 1.96 * (df_Forest[\"f1_std\"] / np.sqrt(10))\n",
    "df_Forest.head()\n",
    "df_Forest = df_Forest.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "df_Forest[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca36a430",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "scaler                  StandardScaler\nestimator       RandomForestClassifier\nimputer                  SimpleImputer\nsampler                            str\nauc_mean                     78.332342\nauc_confl                    78.271965\nauc_confu                    78.392719\ntype2                               79\nf1_mean                       0.775015\nf1_confl                      0.700043\nf1_confu                      0.849987\ndrop_columns                       100\npre_mean                      0.815984\nrec_mean                      0.789526\nName: 15, dtype: object"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Forest = df_Forest.loc[15]\n",
    "Forest[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66a94412",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std  type2  drop_columns            scaler\n0   76.703708  0.059859     73             0      MaxAbsScaler\n3   76.703708  0.059859     73             0      MinMaxScaler\n7   76.703708  0.059859     73             0  PowerTransformer\n10  76.703708  0.059859     73             0      RobustScaler\n13  76.703708  0.059859     73             0    StandardScaler\n16  76.703708  0.059859     73             0               str\n6   75.907765  0.056369     81             0        Normalizer\n2   74.731234  0.047840    105             0      MaxAbsScaler\n5   74.731234  0.047840    105             0      MinMaxScaler\n9   74.731234  0.047840    105             0  PowerTransformer\n12  74.731234  0.047840    105             0      RobustScaler\n15  74.731234  0.047840    105             0    StandardScaler\n18  74.731234  0.047840    105             0               str\n20  74.710316  0.055345    104             0        Normalizer\n1   74.229219  0.049248    100             0      MaxAbsScaler\n4   74.229219  0.049248    100             0      MinMaxScaler\n11  74.229219  0.049248    100             0      RobustScaler\n14  74.229219  0.049248    100             0    StandardScaler\n17  74.229219  0.049248    100             0               str\n19  73.935316  0.051369    103             0        Normalizer",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n      <th>scaler</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73</td>\n      <td>0</td>\n      <td>MaxAbsScaler</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73</td>\n      <td>0</td>\n      <td>MinMaxScaler</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73</td>\n      <td>0</td>\n      <td>PowerTransformer</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73</td>\n      <td>0</td>\n      <td>RobustScaler</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73</td>\n      <td>0</td>\n      <td>StandardScaler</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73</td>\n      <td>0</td>\n      <td>str</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>75.907765</td>\n      <td>0.056369</td>\n      <td>81</td>\n      <td>0</td>\n      <td>Normalizer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>74.731234</td>\n      <td>0.047840</td>\n      <td>105</td>\n      <td>0</td>\n      <td>MaxAbsScaler</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>74.731234</td>\n      <td>0.047840</td>\n      <td>105</td>\n      <td>0</td>\n      <td>MinMaxScaler</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>74.731234</td>\n      <td>0.047840</td>\n      <td>105</td>\n      <td>0</td>\n      <td>PowerTransformer</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>74.731234</td>\n      <td>0.047840</td>\n      <td>105</td>\n      <td>0</td>\n      <td>RobustScaler</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>74.731234</td>\n      <td>0.047840</td>\n      <td>105</td>\n      <td>0</td>\n      <td>StandardScaler</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>74.731234</td>\n      <td>0.047840</td>\n      <td>105</td>\n      <td>0</td>\n      <td>str</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>74.710316</td>\n      <td>0.055345</td>\n      <td>104</td>\n      <td>0</td>\n      <td>Normalizer</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>74.229219</td>\n      <td>0.049248</td>\n      <td>100</td>\n      <td>0</td>\n      <td>MaxAbsScaler</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>74.229219</td>\n      <td>0.049248</td>\n      <td>100</td>\n      <td>0</td>\n      <td>MinMaxScaler</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>74.229219</td>\n      <td>0.049248</td>\n      <td>100</td>\n      <td>0</td>\n      <td>RobustScaler</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>74.229219</td>\n      <td>0.049248</td>\n      <td>100</td>\n      <td>0</td>\n      <td>StandardScaler</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>74.229219</td>\n      <td>0.049248</td>\n      <td>100</td>\n      <td>0</td>\n      <td>str</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>73.935316</td>\n      <td>0.051369</td>\n      <td>103</td>\n      <td>0</td>\n      <td>Normalizer</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading Decision Tree Classifier\n",
    "with open(\"outputs/output DecisionTrees.json\", 'r') as f:\n",
    "  data_Tree = json.load(f)\n",
    "\n",
    "df_Tree = pd.json_normalize(data_Tree, record_path =['measurements'])\n",
    "df_Tree[\"drop_columns\"] = df_Tree[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "df_Tree[\"auc_std\"] = df_Tree.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "df_Tree[\"pre_mean\"] = df_Tree.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "df_Tree[\"rec_mean\"] = df_Tree.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "df_Tree[\"f1_std\"] = df_Tree.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_Tree[\"f1_mean\"] = df_Tree.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_Tree[\"type2\"] = df_Tree.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "df_Tree[\"auc_confl\"] = df_Tree[\"auc_mean\"]- 1.96 * (df_Tree[\"auc_std\"] / np.sqrt(10))\n",
    "df_Tree[\"auc_confu\"] = df_Tree[\"auc_mean\"]+ 1.96 * (df_Tree[\"auc_std\"] / np.sqrt(10))\n",
    "df_Tree[\"f1_confl\"] = df_Tree[\"f1_mean\"]- 1.96 * (df_Tree[\"f1_std\"] / np.sqrt(10))\n",
    "df_Tree[\"f1_confu\"] = df_Tree[\"f1_mean\"]+ 1.96 * (df_Tree[\"f1_std\"] / np.sqrt(10))\n",
    "df_Tree.head()\n",
    "df_Tree = df_Tree.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "df_Tree[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\", 'scaler']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bbae235",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "scaler                             str\nestimator       DecisionTreeClassifier\nimputer                  SimpleImputer\nsampler                            str\nauc_mean                     76.703708\nauc_confl                    76.666607\nauc_confu                     76.74081\ntype2                               73\nf1_mean                       0.770171\nf1_confl                      0.733067\nf1_confu                      0.807274\ndrop_columns                         0\npre_mean                      0.785349\nrec_mean                      0.776305\nName: 16, dtype: object"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree = df_Tree.loc[16]\n",
    "\n",
    "Tree[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26ecfdd7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Tree.to_excel(r'DTree.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11bba66a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std  type2  drop_columns\n17  79.468591  0.065352  101.0          20.0\n2   78.508412  0.076647  113.0         100.0\n16  78.051257  0.084012  110.0          20.0\n0   77.935901  0.073471   94.0          20.0\n4   77.907379  0.083855  114.0          20.0\n9   77.870632  0.084183   92.0          20.0\n3   77.527738  0.073300   98.0          20.0\n11  77.283387  0.075887  114.0          75.0\n5   77.206434  0.071671  133.0          75.0\n15  77.173656  0.078727  104.0          20.0\n1   77.007354  0.093415  113.0          20.0\n10  76.849689  0.079539  111.0          20.0\n6   76.793703  0.070966  106.0          60.0\n8   76.439149  0.067503  138.0          20.0\n7   75.542198  0.067355  147.0          20.0\n12        NaN       NaN    NaN           NaN\n13        NaN       NaN    NaN           NaN\n14        NaN       NaN    NaN           NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>79.468591</td>\n      <td>0.065352</td>\n      <td>101.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>78.508412</td>\n      <td>0.076647</td>\n      <td>113.0</td>\n      <td>100.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>78.051257</td>\n      <td>0.084012</td>\n      <td>110.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>77.935901</td>\n      <td>0.073471</td>\n      <td>94.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>77.907379</td>\n      <td>0.083855</td>\n      <td>114.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>77.870632</td>\n      <td>0.084183</td>\n      <td>92.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>77.527738</td>\n      <td>0.073300</td>\n      <td>98.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>77.283387</td>\n      <td>0.075887</td>\n      <td>114.0</td>\n      <td>75.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>77.206434</td>\n      <td>0.071671</td>\n      <td>133.0</td>\n      <td>75.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>77.173656</td>\n      <td>0.078727</td>\n      <td>104.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>77.007354</td>\n      <td>0.093415</td>\n      <td>113.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>76.849689</td>\n      <td>0.079539</td>\n      <td>111.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>76.793703</td>\n      <td>0.070966</td>\n      <td>106.0</td>\n      <td>60.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>76.439149</td>\n      <td>0.067503</td>\n      <td>138.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75.542198</td>\n      <td>0.067355</td>\n      <td>147.0</td>\n      <td>20.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading SVC\n",
    "with open(\"outputs/output SVC.json\", 'r') as f:\n",
    "  data_SVC = json.load(f)\n",
    "\n",
    "df_SVC = pd.json_normalize(data_SVC, record_path =['measurements'])\n",
    "df_SVC[\"drop_columns\"] = df_SVC[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "df_SVC[\"auc_std\"] = df_SVC.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "df_SVC[\"pre_mean\"] = df_SVC.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "df_SVC[\"rec_mean\"] = df_SVC.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "df_SVC[\"f1_std\"] = df_SVC.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_SVC[\"f1_mean\"] = df_SVC.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_SVC[\"type2\"] = df_SVC.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "df_SVC[\"auc_confl\"] = df_SVC[\"auc_mean\"]- 1.96 * (df_SVC[\"auc_std\"] / np.sqrt(10))\n",
    "df_SVC[\"auc_confu\"] = df_SVC[\"auc_mean\"]+ 1.96 * (df_SVC[\"auc_std\"] / np.sqrt(10))\n",
    "df_SVC[\"f1_confl\"] = df_SVC[\"f1_mean\"]- 1.96 * (df_SVC[\"f1_std\"] / np.sqrt(10))\n",
    "df_SVC[\"f1_confu\"] = df_SVC[\"f1_mean\"]+ 1.96 * (df_SVC[\"f1_std\"] / np.sqrt(10))\n",
    "df_SVC.head()\n",
    "df_SVC = df_SVC.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "df_SVC[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "213a7155",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "scaler          PowerTransformer\nestimator                    SVC\nimputer            SimpleImputer\nsampler                      str\nauc_mean               77.870632\nauc_confl              77.818455\nauc_confu              77.922809\ntype2                       92.0\nf1_mean                 0.774087\nf1_confl                0.716995\nf1_confu                0.831179\ndrop_columns                20.0\npre_mean                0.800496\nrec_mean                0.782909\nName: 9, dtype: object"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC = df_SVC.loc[9]\n",
    "SVC[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c2b2fc0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std  type2  drop_columns\n15  79.068118  0.071319    111             8\n16  78.913029  0.067901    115             8\n17  78.758947  0.069558    119             8\n9   77.518143  0.095385    109            35\n10  77.389062  0.094922    114            35\n11  77.384980  0.097225    114            35\n12  76.922710  0.099803    116            60\n0   76.840119  0.091046    102            20\n6   76.840119  0.091046    102            20\n18  76.840119  0.091046    102            20\n4   76.579355  0.079567    111             8\n3   76.230836  0.077197    101             8\n5   76.179355  0.079506    115             8\n2   76.120744  0.081529    119            20\n8   76.120744  0.081529    119            20\n20  76.120744  0.081529    119            20\n13  76.084457  0.108448    123            60\n14  75.883425  0.103597    125            60\n1   75.379915  0.085676    119            20\n7   75.379915  0.085676    119            20",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>79.068118</td>\n      <td>0.071319</td>\n      <td>111</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>78.913029</td>\n      <td>0.067901</td>\n      <td>115</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>78.758947</td>\n      <td>0.069558</td>\n      <td>119</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>77.518143</td>\n      <td>0.095385</td>\n      <td>109</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>77.389062</td>\n      <td>0.094922</td>\n      <td>114</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>77.384980</td>\n      <td>0.097225</td>\n      <td>114</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>76.922710</td>\n      <td>0.099803</td>\n      <td>116</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>76.840119</td>\n      <td>0.091046</td>\n      <td>102</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>76.840119</td>\n      <td>0.091046</td>\n      <td>102</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>76.840119</td>\n      <td>0.091046</td>\n      <td>102</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>76.579355</td>\n      <td>0.079567</td>\n      <td>111</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>76.230836</td>\n      <td>0.077197</td>\n      <td>101</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>76.179355</td>\n      <td>0.079506</td>\n      <td>115</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>76.120744</td>\n      <td>0.081529</td>\n      <td>119</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>76.120744</td>\n      <td>0.081529</td>\n      <td>119</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>76.120744</td>\n      <td>0.081529</td>\n      <td>119</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>76.084457</td>\n      <td>0.108448</td>\n      <td>123</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>75.883425</td>\n      <td>0.103597</td>\n      <td>125</td>\n      <td>60</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>75.379915</td>\n      <td>0.085676</td>\n      <td>119</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75.379915</td>\n      <td>0.085676</td>\n      <td>119</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading naÃ¯ve bayes (bernoulli)\n",
    "with open(\"outputs/output BernoulliNB.json\", 'r') as f:\n",
    "  data_bernoulli = json.load(f)\n",
    "\n",
    "df_bernoulli = pd.json_normalize(data_bernoulli, record_path =['measurements'])\n",
    "df_bernoulli[\"drop_columns\"] = df_bernoulli[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "df_bernoulli[\"auc_std\"] = df_bernoulli.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "df_bernoulli[\"pre_mean\"] = df_bernoulli.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "df_bernoulli[\"rec_mean\"] = df_bernoulli.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "df_bernoulli[\"f1_std\"] = df_bernoulli.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_bernoulli[\"f1_mean\"] = df_bernoulli.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_bernoulli[\"type2\"] = df_bernoulli.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "df_bernoulli[\"auc_confl\"] = df_bernoulli[\"auc_mean\"]- 1.96 * (df_bernoulli[\"auc_std\"] / np.sqrt(10))\n",
    "df_bernoulli[\"auc_confu\"] = df_bernoulli[\"auc_mean\"]+ 1.96 * (df_bernoulli[\"auc_std\"] / np.sqrt(10))\n",
    "df_bernoulli[\"f1_confl\"] = df_bernoulli[\"f1_mean\"]- 1.96 * (df_bernoulli[\"f1_std\"] / np.sqrt(10))\n",
    "df_bernoulli[\"f1_confu\"] = df_bernoulli[\"f1_mean\"]+ 1.96 * (df_bernoulli[\"f1_std\"] / np.sqrt(10))\n",
    "df_bernoulli.head()\n",
    "df_bernoulli = df_bernoulli.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "df_bernoulli[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58b80725",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "scaler           MinMaxScaler\nestimator         BernoulliNB\nimputer         SimpleImputer\nsampler                   str\nauc_mean            76.230836\nauc_confl           76.182989\nauc_confu           76.278684\ntype2                     101\nf1_mean              0.757561\nf1_confl             0.704142\nf1_confu              0.81098\ndrop_columns                8\npre_mean             0.779828\nrec_mean              0.76623\nName: 3, dtype: object"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli = df_bernoulli.loc[3]\n",
    "bernoulli[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58649a95",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'auc'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3629\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3628\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3630\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'auc'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [28], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m df_categorical \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mjson_normalize(data_categorical, record_path \u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmeasurements\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      6\u001B[0m df_categorical[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrop_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_XGBoost[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_params.drop_columns__minimum_percentage_to_be_dropped\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 7\u001B[0m df_categorical[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauc_std\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_categorical\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: np\u001B[38;5;241m.\u001B[39mstd(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauc\u001B[39m\u001B[38;5;124m\"\u001B[39m]), axis \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      8\u001B[0m df_categorical[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpre_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_categorical\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: np\u001B[38;5;241m.\u001B[39mmean(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassification_report.weighted avg.precision\u001B[39m\u001B[38;5;124m\"\u001B[39m]), axis \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      9\u001B[0m df_categorical[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrec_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_categorical\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: np\u001B[38;5;241m.\u001B[39mmean(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassification_report.weighted avg.recall\u001B[39m\u001B[38;5;124m\"\u001B[39m]), axis \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:8848\u001B[0m, in \u001B[0;36mDataFrame.apply\u001B[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001B[0m\n\u001B[1;32m   8837\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapply\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m frame_apply\n\u001B[1;32m   8839\u001B[0m op \u001B[38;5;241m=\u001B[39m frame_apply(\n\u001B[1;32m   8840\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   8841\u001B[0m     func\u001B[38;5;241m=\u001B[39mfunc,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   8846\u001B[0m     kwargs\u001B[38;5;241m=\u001B[39mkwargs,\n\u001B[1;32m   8847\u001B[0m )\n\u001B[0;32m-> 8848\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m__finalize__(\u001B[38;5;28mself\u001B[39m, method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapply\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:733\u001B[0m, in \u001B[0;36mFrameApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    730\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw:\n\u001B[1;32m    731\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_raw()\n\u001B[0;32m--> 733\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:857\u001B[0m, in \u001B[0;36mFrameApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    856\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply_standard\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 857\u001B[0m     results, res_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_series_generator\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    859\u001B[0m     \u001B[38;5;66;03m# wrap results\u001B[39;00m\n\u001B[1;32m    860\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrap_results(results, res_index)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/apply.py:873\u001B[0m, in \u001B[0;36mFrameApply.apply_series_generator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    870\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    871\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(series_gen):\n\u001B[1;32m    872\u001B[0m         \u001B[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001B[39;00m\n\u001B[0;32m--> 873\u001B[0m         results[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    874\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(results[i], ABCSeries):\n\u001B[1;32m    875\u001B[0m             \u001B[38;5;66;03m# If we have a view on v, we need to make a copy because\u001B[39;00m\n\u001B[1;32m    876\u001B[0m             \u001B[38;5;66;03m#  series_generator will swap out the underlying data\u001B[39;00m\n\u001B[1;32m    877\u001B[0m             results[i] \u001B[38;5;241m=\u001B[39m results[i]\u001B[38;5;241m.\u001B[39mcopy(deep\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn [28], line 7\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(row)\u001B[0m\n\u001B[1;32m      5\u001B[0m df_categorical \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mjson_normalize(data_categorical, record_path \u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmeasurements\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      6\u001B[0m df_categorical[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdrop_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_XGBoost[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbest_params.drop_columns__minimum_percentage_to_be_dropped\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m----> 7\u001B[0m df_categorical[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauc_std\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_categorical\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: np\u001B[38;5;241m.\u001B[39mstd(\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m), axis \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      8\u001B[0m df_categorical[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpre_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_categorical\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: np\u001B[38;5;241m.\u001B[39mmean(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassification_report.weighted avg.precision\u001B[39m\u001B[38;5;124m\"\u001B[39m]), axis \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      9\u001B[0m df_categorical[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrec_mean\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m df_categorical\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m row: np\u001B[38;5;241m.\u001B[39mmean(row[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclassification_report.weighted avg.recall\u001B[39m\u001B[38;5;124m\"\u001B[39m]), axis \u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:958\u001B[0m, in \u001B[0;36mSeries.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    955\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[key]\n\u001B[1;32m    957\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[0;32m--> 958\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_hashable(key):\n\u001B[1;32m    961\u001B[0m     \u001B[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001B[39;00m\n\u001B[1;32m    962\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    963\u001B[0m         \u001B[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/series.py:1069\u001B[0m, in \u001B[0;36mSeries._get_value\u001B[0;34m(self, label, takeable)\u001B[0m\n\u001B[1;32m   1066\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_values[label]\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[0;32m-> 1069\u001B[0m loc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1070\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex\u001B[38;5;241m.\u001B[39m_get_values_for_loc(\u001B[38;5;28mself\u001B[39m, loc, label)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3631\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3629\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3630\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3631\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3632\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3633\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3634\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3635\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3636\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'auc'"
     ]
    }
   ],
   "source": [
    "# # loading naÃ¯ve bayes (categorical)\n",
    "# with open(\"outputs/output CategoricalNB.json\", 'r') as f:\n",
    "#   data_categorical = json.load(f)\n",
    "#\n",
    "# df_categorical = pd.json_normalize(data_categorical, record_path =['measurements'])\n",
    "# df_categorical[\"drop_columns\"] = df_XGBoost[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "# df_categorical[\"auc_std\"] = df_categorical.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "# df_categorical[\"pre_mean\"] = df_categorical.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "# df_categorical[\"rec_mean\"] = df_categorical.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "# df_categorical[\"f1_std\"] = df_categorical.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "# df_categorical[\"f1_mean\"] = df_categorical.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "# df_categorical[\"type2\"] = df_categorical.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "# df_categorical[\"auc_confl\"] = df_categorical[\"auc_mean\"]- 1.96 * (df_categorical[\"auc_std\"] / np.sqrt(10))\n",
    "# df_categorical[\"auc_confu\"] = df_categorical[\"auc_mean\"]+ 1.96 * (df_categorical[\"auc_std\"] / np.sqrt(10))\n",
    "# df_categorical[\"f1_confl\"] = df_categorical[\"f1_mean\"]- 1.96 * (df_categorical[\"f1_std\"] / np.sqrt(10))\n",
    "# df_categorical[\"f1_confu\"] = df_categorical[\"f1_mean\"]+ 1.96 * (df_categorical[\"f1_std\"] / np.sqrt(10))\n",
    "# df_categorical.head()\n",
    "# df_categorical = df_categorical.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "# df_categorical[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ef3eb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# categorical = df_categorical.loc[0]\n",
    "# categorical[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80699449",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # loading naÃ¯ve bayes (complement)\n",
    "# with open(\"outputs/output CompleteNB.json\", 'r') as f:\n",
    "#   data_complement = json.load(f)\n",
    "#\n",
    "# df_complement = pd.json_normalize(data_complement, record_path =['measurements'])\n",
    "# df_complement[\"drop_columns\"] = df_complement[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "# df_complement[\"auc_std\"] = df_complement.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "# df_complement[\"pre_mean\"] = df_complement.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "# df_complement[\"rec_mean\"] = df_complement.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "# df_complement[\"f1_std\"] = df_complement.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "# df_complement[\"f1_mean\"] = df_complement.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "# df_complement[\"type2\"] = df_complement.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "# df_complement[\"auc_confl\"] = df_complement[\"auc_mean\"]- 1.96 * (df_complement[\"auc_std\"] / np.sqrt(10))\n",
    "# df_complement[\"auc_confu\"] = df_complement[\"auc_mean\"]+ 1.96 * (df_complement[\"auc_std\"] / np.sqrt(10))\n",
    "# df_complement[\"f1_confl\"] = df_complement[\"f1_mean\"]- 1.96 * (df_complement[\"f1_std\"] / np.sqrt(10))\n",
    "# df_complement[\"f1_confu\"] = df_complement[\"f1_mean\"]+ 1.96 * (df_complement[\"f1_std\"] / np.sqrt(10))\n",
    "# df_complement.head()\n",
    "# df_complement = df_complement.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "# df_complement[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c242b21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# complement = df_complement.loc[3]\n",
    "# complement[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fb733c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # loading naÃ¯ve bayes (gaussian)\n",
    "# with open(\"outputs/output GaussianNB.json\", 'r') as f:\n",
    "#   data_gaussian = json.load(f)\n",
    "#\n",
    "# df_gaussian = pd.json_normalize(data_gaussian, record_path =['measurements'])\n",
    "# df_gaussian[\"drop_columns\"] = df_gaussian[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "# df_gaussian[\"auc_std\"] = df_gaussian.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "# df_gaussian[\"pre_mean\"] = df_gaussian.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "# df_gaussian[\"rec_mean\"] = df_gaussian.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "# df_gaussian[\"f1_std\"] = df_gaussian.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "# df_gaussian[\"f1_mean\"] = df_gaussian.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "# df_gaussian[\"type2\"] = df_gaussian.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "# df_gaussian[\"auc_confl\"] = df_gaussian[\"auc_mean\"]- 1.96 * (df_gaussian[\"auc_std\"] / np.sqrt(10))\n",
    "# df_gaussian[\"auc_confu\"] = df_gaussian[\"auc_mean\"]+ 1.96 * (df_gaussian[\"auc_std\"] / np.sqrt(10))\n",
    "# df_gaussian[\"f1_confl\"] = df_gaussian[\"f1_mean\"]- 1.96 * (df_gaussian[\"f1_std\"] / np.sqrt(10))\n",
    "# df_gaussian[\"f1_confu\"] = df_gaussian[\"f1_mean\"]+ 1.96 * (df_gaussian[\"f1_std\"] / np.sqrt(10))\n",
    "# df_gaussian.head()\n",
    "# df_gaussian = df_gaussian.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "# df_gaussian[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a6091",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# gaussian = df_gaussian.loc[20]\n",
    "# gaussian[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff1c61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # loading naÃ¯ve bayes (multinomial)\n",
    "# with open(\"outputs/output MultinomialNB.json\", 'r') as f:\n",
    "#   data_multinomial = json.load(f)\n",
    "#\n",
    "# df_multinomial = pd.json_normalize(data_multinomial, record_path =['measurements'])\n",
    "# df_multinomial[\"drop_columns\"] = df_multinomial[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "# df_multinomial[\"auc_std\"] = df_multinomial.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "# df_multinomial[\"pre_mean\"] = df_multinomial.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "# df_multinomial[\"rec_mean\"] = df_multinomial.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "# df_multinomial[\"f1_std\"] = df_multinomial.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "# df_multinomial[\"f1_mean\"] = df_multinomial.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "# df_multinomial[\"type2\"] = df_multinomial.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "# df_multinomial[\"auc_confl\"] = df_multinomial[\"auc_mean\"]- 1.96 * (df_multinomial[\"auc_std\"] / np.sqrt(10))\n",
    "# df_multinomial[\"auc_confu\"] = df_multinomial[\"auc_mean\"]+ 1.96 * (df_multinomial[\"auc_std\"] / np.sqrt(10))\n",
    "# df_multinomial[\"f1_confl\"] = df_multinomial[\"f1_mean\"]- 1.96 * (df_multinomial[\"f1_std\"] / np.sqrt(10))\n",
    "# df_multinomial[\"f1_confu\"] = df_multinomial[\"f1_mean\"]+ 1.96 * (df_multinomial[\"f1_std\"] / np.sqrt(10))\n",
    "# df_multinomial.head()\n",
    "# df_multinomial = df_multinomial.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "# df_multinomial[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7976884",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# multinomial = df_multinomial.loc[3]\n",
    "# multinomial[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1abf8371",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std  type2  drop_columns\n17  78.818143  0.081598    111            20\n14  78.633972  0.081839    114            20\n2   78.319126  0.066004    116            20\n3   77.893591  0.077889    103            20\n16  77.696192  0.072278    116            20\n13  77.665045  0.079287    120            20\n19  77.646167  0.089447    114            20\n15  77.641575  0.069453    103            20\n12  77.563502  0.064914    105            35\n5   77.563004  0.069132    121            20\n20  77.523768  0.090437    120            20\n11  77.269151  0.069457    119            20\n1   77.198183  0.080673    116            20\n4   77.141053  0.082720    119            20\n0   77.139510  0.072828    103            20\n18  76.886461  0.083706    103            20\n7   76.431819  0.053061    114             0\n8   76.283860  0.052359    118             0\n6   76.028148  0.057076     75             0\n10  75.964037  0.094453    127            20",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>17</th>\n      <td>78.818143</td>\n      <td>0.081598</td>\n      <td>111</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>78.633972</td>\n      <td>0.081839</td>\n      <td>114</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>78.319126</td>\n      <td>0.066004</td>\n      <td>116</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>77.893591</td>\n      <td>0.077889</td>\n      <td>103</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>77.696192</td>\n      <td>0.072278</td>\n      <td>116</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>77.665045</td>\n      <td>0.079287</td>\n      <td>120</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>77.646167</td>\n      <td>0.089447</td>\n      <td>114</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>77.641575</td>\n      <td>0.069453</td>\n      <td>103</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>77.563502</td>\n      <td>0.064914</td>\n      <td>105</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>77.563004</td>\n      <td>0.069132</td>\n      <td>121</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>77.523768</td>\n      <td>0.090437</td>\n      <td>120</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>77.269151</td>\n      <td>0.069457</td>\n      <td>119</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>77.198183</td>\n      <td>0.080673</td>\n      <td>116</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>77.141053</td>\n      <td>0.082720</td>\n      <td>119</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>77.139510</td>\n      <td>0.072828</td>\n      <td>103</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>76.886461</td>\n      <td>0.083706</td>\n      <td>103</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>76.431819</td>\n      <td>0.053061</td>\n      <td>114</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>76.283860</td>\n      <td>0.052359</td>\n      <td>118</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>76.028148</td>\n      <td>0.057076</td>\n      <td>75</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>75.964037</td>\n      <td>0.094453</td>\n      <td>127</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading logistic regression\n",
    "with open(\"outputs/output LogisticRegression.json\", 'r') as f:\n",
    "  data_logistic = json.load(f)\n",
    "\n",
    "df_logistic = pd.json_normalize(data_logistic, record_path =['measurements'])\n",
    "df_logistic[\"drop_columns\"] = df_logistic[\"best_params.drop_columns__minimum_percentage_to_be_dropped\"]\n",
    "df_logistic[\"auc_std\"] = df_logistic.apply(lambda row: np.std(row[\"auc\"]), axis =1)\n",
    "df_logistic[\"pre_mean\"] = df_logistic.apply(lambda row: np.mean(row[\"classification_report.weighted avg.precision\"]), axis =1)\n",
    "df_logistic[\"rec_mean\"] = df_logistic.apply(lambda row: np.mean(row[\"classification_report.weighted avg.recall\"]), axis =1)\n",
    "df_logistic[\"f1_std\"] = df_logistic.apply(lambda row: np.std(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_logistic[\"f1_mean\"] = df_logistic.apply(lambda row: np.mean(row[\"classification_report.weighted avg.f1-score\"]), axis =1)\n",
    "df_logistic[\"type2\"] = df_logistic.apply(lambda row: np.sum(row[\"confusion_matrix.(1, 0)\"]), axis =1)\n",
    "df_logistic[\"auc_confl\"] = df_logistic[\"auc_mean\"]- 1.96 * (df_logistic[\"auc_std\"] / np.sqrt(10))\n",
    "df_logistic[\"auc_confu\"] = df_logistic[\"auc_mean\"]+ 1.96 * (df_logistic[\"auc_std\"] / np.sqrt(10))\n",
    "df_logistic[\"f1_confl\"] = df_logistic[\"f1_mean\"]- 1.96 * (df_logistic[\"f1_std\"] / np.sqrt(10))\n",
    "df_logistic[\"f1_confu\"] = df_logistic[\"f1_mean\"]+ 1.96 * (df_logistic[\"f1_std\"] / np.sqrt(10))\n",
    "df_logistic.head()\n",
    "df_logistic = df_logistic.sort_values(by = [\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"], ascending = False)\n",
    "df_logistic[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\"]].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "df = pd.concat([df_Tree, df_XGBoost, df_SVC, df_logistic, df_Forest, df_bernoulli, df_KNeighbors])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std  type2  drop_columns               estimator\n18  76.285254  0.056878   70.0           0.0    KNeighborsClassifier\n0   76.703708  0.059859   73.0           0.0  DecisionTreeClassifier\n3   76.703708  0.059859   73.0           0.0  DecisionTreeClassifier\n7   76.703708  0.059859   73.0           0.0  DecisionTreeClassifier\n10  76.703708  0.059859   73.0           0.0  DecisionTreeClassifier\n..        ...       ...    ...           ...                     ...\n8   76.439149  0.067503  138.0          20.0                     SVC\n7   75.542198  0.067355  147.0          20.0                     SVC\n12        NaN       NaN    NaN           NaN                     SVC\n13        NaN       NaN    NaN           NaN                     SVC\n14        NaN       NaN    NaN           NaN                     SVC\n\n[141 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n      <th>estimator</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>18</th>\n      <td>76.285254</td>\n      <td>0.056878</td>\n      <td>70.0</td>\n      <td>0.0</td>\n      <td>KNeighborsClassifier</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73.0</td>\n      <td>0.0</td>\n      <td>DecisionTreeClassifier</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73.0</td>\n      <td>0.0</td>\n      <td>DecisionTreeClassifier</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73.0</td>\n      <td>0.0</td>\n      <td>DecisionTreeClassifier</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>76.703708</td>\n      <td>0.059859</td>\n      <td>73.0</td>\n      <td>0.0</td>\n      <td>DecisionTreeClassifier</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>76.439149</td>\n      <td>0.067503</td>\n      <td>138.0</td>\n      <td>20.0</td>\n      <td>SVC</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>75.542198</td>\n      <td>0.067355</td>\n      <td>147.0</td>\n      <td>20.0</td>\n      <td>SVC</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>SVC</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>SVC</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>SVC</td>\n    </tr>\n  </tbody>\n</table>\n<p>141 rows Ã— 5 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"auc_mean\", \"auc_std\", \"type2\", \"drop_columns\", \"estimator\"]].sort_values('type2')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "     auc_mean   auc_std   f1_mean  type2  drop_columns               estimator\n15  80.398731  0.097697  0.797751   83.0         100.0    KNeighborsClassifier\n17  79.468591  0.065352  0.792116  101.0          20.0                     SVC\n17  79.798258  0.090229  0.791528  100.0         100.0    KNeighborsClassifier\n15  79.068118  0.071319  0.787344  111.0           8.0             BernoulliNB\n16  79.383997  0.092650  0.786896  109.0         100.0    KNeighborsClassifier\n..        ...       ...       ...    ...           ...                     ...\n19  73.935316  0.051369  0.739903  103.0           0.0  DecisionTreeClassifier\n8   73.366974  0.067874  0.734026  106.0           0.0  DecisionTreeClassifier\n12        NaN       NaN       NaN    NaN           NaN                     SVC\n13        NaN       NaN       NaN    NaN           NaN                     SVC\n14        NaN       NaN       NaN    NaN           NaN                     SVC\n\n[141 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>auc_mean</th>\n      <th>auc_std</th>\n      <th>f1_mean</th>\n      <th>type2</th>\n      <th>drop_columns</th>\n      <th>estimator</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>80.398731</td>\n      <td>0.097697</td>\n      <td>0.797751</td>\n      <td>83.0</td>\n      <td>100.0</td>\n      <td>KNeighborsClassifier</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>79.468591</td>\n      <td>0.065352</td>\n      <td>0.792116</td>\n      <td>101.0</td>\n      <td>20.0</td>\n      <td>SVC</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>79.798258</td>\n      <td>0.090229</td>\n      <td>0.791528</td>\n      <td>100.0</td>\n      <td>100.0</td>\n      <td>KNeighborsClassifier</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>79.068118</td>\n      <td>0.071319</td>\n      <td>0.787344</td>\n      <td>111.0</td>\n      <td>8.0</td>\n      <td>BernoulliNB</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>79.383997</td>\n      <td>0.092650</td>\n      <td>0.786896</td>\n      <td>109.0</td>\n      <td>100.0</td>\n      <td>KNeighborsClassifier</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>73.935316</td>\n      <td>0.051369</td>\n      <td>0.739903</td>\n      <td>103.0</td>\n      <td>0.0</td>\n      <td>DecisionTreeClassifier</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>73.366974</td>\n      <td>0.067874</td>\n      <td>0.734026</td>\n      <td>106.0</td>\n      <td>0.0</td>\n      <td>DecisionTreeClassifier</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>SVC</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>SVC</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>SVC</td>\n    </tr>\n  </tbody>\n</table>\n<p>141 rows Ã— 6 columns</p>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"auc_mean\", \"auc_std\",'f1_mean', \"type2\", \"drop_columns\", \"estimator\"]].sort_values('f1_mean', ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfeb9aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Tree = df_Tree.loc[16]\n",
    "\n",
    "Tree[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5cf10",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "logistic = df_logistic.loc[6]\n",
    "logistic[[\"scaler\", \"estimator\", \"imputer\", \"sampler\", \"auc_mean\", \"auc_confl\", \"auc_confu\", \"type2\", \"f1_mean\", \"f1_confl\", \"f1_confu\", \"drop_columns\", \"pre_mean\", \"rec_mean\"]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "f335c2cb84fafae796dd7ed83c640fd2cf2129dda2b9a14a13240a0604884ad1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}